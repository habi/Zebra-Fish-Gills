{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the gills\n",
    "- Oleksiy and Flu scanned them\n",
    "- Dea delineated them\n",
    "- David does the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "seaborn.set_style('dark')\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import os\n",
    "import platform\n",
    "import pandas\n",
    "import glob\n",
    "import scipy.misc\n",
    "import imageio\n",
    "import scipy.stats\n",
    "import numpy\n",
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import gc\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style plots\n",
    "seaborn.set_style('whitegrid')\n",
    "seaborn.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and output defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "# plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['savefig.dpi'] = 300  # Save (PNG) images with a higher DPI, since Authorea cannot import PDFs...\n",
    "plt.rcParams[\"savefig.transparent\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    \"\"\"\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    \"\"\"\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git', '--git-dir', os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse', '--short', '--verify', 'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_gray_values(logfile):\n",
    "    \"\"\"Get the low and high value for the mapping of the reconstruction gray values\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Minimum for CS' in line:\n",
    "                minimum = float(line.split('=')[1])\n",
    "            if 'Maximum for CS' in line:\n",
    "                maximum = float(line.split('=')[1])\n",
    "    return(minimum, maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whodunit(logfile):\n",
    "    \"\"\"\n",
    "    Who did the scan?\n",
    "    https://en.wikipedia.org/wiki/Whodunit\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'User Name' in line:\n",
    "                user = line.split('=')[1].rstrip()\n",
    "    return(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(value1, value2):\n",
    "    \"\"\"\n",
    "    We use this over and over in the results part.\n",
    "    Just a helper function to print out the percentage increase.\n",
    "    \"\"\"\n",
    "    p = value2 / value1\n",
    "    p -= 1\n",
    "    p *= 100\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(p):\n",
    "    \"\"\"\n",
    "    Get some 'information' on the p-values we calculate.\n",
    "    See the bottom of https://git.io/vQbWV\n",
    "    \"\"\"\n",
    "    if p < 0.0001:\n",
    "        return \"****\" % p\n",
    "        # The function used to return (\"**** (p=%0.2g)\" % p),\n",
    "        # but we try to reduce clutter in the plots and only specify the level now\n",
    "    elif (p < 0.001):\n",
    "        return \"***\" % p\n",
    "    elif (p < 0.01):\n",
    "        return \"**\" % p\n",
    "    elif (p < 0.05):\n",
    "        return \"*\" % p\n",
    "    else:\n",
    "        return \"n.s.\" % p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance_bar(start, end, height, pvalue, markersize=25, boxpad=0, color='gray'):\n",
    "    \"\"\" Draw significance bars, based on on  https://stackoverflow.com/a/33873837/323100 \"\"\"\n",
    "    # Draw a line with downticks at the ends\n",
    "    plt.plot([start, end],\n",
    "             [1.05 * height] * 2,\n",
    "             '-',\n",
    "             color=color,\n",
    "             marker=3,  # 3 == TICKDOWN: https://matplotlib.org/api/markers_api.html\n",
    "             markersize=markersize)\n",
    "    # Draw the text with semitransparent background in case we cover other plots\n",
    "    plt.text(0.5 * (start + end),\n",
    "             1.05 * height,\n",
    "             '%s' % significance(pvalue),\n",
    "             ha='center',\n",
    "             va='bottom',\n",
    "             bbox=dict(facecolor='white',\n",
    "                       alpha=0.309,\n",
    "                       edgecolor='none',\n",
    "                       boxstyle='Square,pad=' + str(boxpad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on the fast SSD from here on!\n",
    "FastSSD = True\n",
    "if 'debian' in platform.dist():\n",
    "    if FastSSD:\n",
    "        StartDir = '/media/habi/Fast_SSD/'\n",
    "    else:\n",
    "        StartDir = '/media/habi/Blue Seagate/'\n",
    "else:\n",
    "    if FastSSD:\n",
    "        StartDir = 'F:/'\n",
    "    else:\n",
    "        StartDir = 'G:/'\n",
    "RootFolder = os.path.join(StartDir, 'Zebra-Fish_Matthias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are loading all the data from %s' % RootFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving the output images to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 5\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(ROIFolder) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get going, now that we set up everything..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the microCT scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the samples we scanned\n",
    "try:\n",
    "    SampleNames = sorted(next(os.walk(RootFolder))[1])\n",
    "except StopIteration:\n",
    "    print('Please mount the fast SSD!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Folder we don't want\n",
    "if not FastSSD:\n",
    "    SampleNames.remove('Original SEM Bilder')\n",
    "    SampleNames.remove('tresholding_estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the information into a pandas dataframe\n",
    "Data = pandas.DataFrame({'Sample': SampleNames})\n",
    "Data['Folder'] = [os.path.join(RootFolder, s) for s in SampleNames]\n",
    "Data['LogFile'] = [sorted(glob.glob(os.path.join(f, 'proj', '*.log')))[0] for f in Data.Folder]\n",
    "Data['RecFolder'] = [os.path.join(RootFolder, f, 'rec') for f in Data.Folder]\n",
    "Data['VOIFolder'] = [os.path.join(RootFolder, f, 'VOI') for f in Data.Folder]\n",
    "Data['OverviewName'] = [glob.glob(os.path.join(r, '*spr.bmp'))[0] for r in Data.RecFolder]\n",
    "Data['ReconstructionNames'] = [sorted(glob.glob(os.path.join(r, '*.png'))) for r in Data.RecFolder]\n",
    "# Try to be a bit clever with loading the VOI slices\n",
    "# They are saved as either PNG or BMP, and there's also some other stuff in the folder...\n",
    "Data['VOINames'] = [sorted(glob.glob(os.path.join(r, '*rec*[0123456789].*'))) for r in Data.VOIFolder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an 'experiment' column, which we use for the box plots below\n",
    "def whichexperiment(i):\n",
    "    '''Categorize  into 'Swimmer' or 'Control' '''\n",
    "    if 'immer' in i:\n",
    "        return 'Swimmer'\n",
    "    if 'ontrol' in i:\n",
    "        return 'Control'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Experiment'] = [whichexperiment(name) for name in Data.Folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color plot based on label (mrt, ctrl or bb)\n",
    "def color_based_on_experiment(i):\n",
    "    '''Colorize into 'Swimmer' or 'Control' '''\n",
    "    if 'ontrol' in i:\n",
    "        return seaborn.color_palette()[0]\n",
    "    if 'immer' in i:\n",
    "        return seaborn.color_palette()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Color'] = [color_based_on_experiment(name) for name in Data.Folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pixel sizes of the scans\n",
    "# in micrometers\n",
    "Data['PixelSize'] = [get_pixelsize(logfile) for logfile in Data.LogFile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pixel sizes of the scans\n",
    "# in micrometers\n",
    "Data['ScanMaster'] = [whodunit(logfile) for logfile in Data.LogFile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who did what?\n",
    "print('From the %s samples we have' % len(Data))\n",
    "for who in Data.ScanMaster.unique():\n",
    "    print('\\t- %s scanned %s samples' % (who, len(Data[Data.ScanMaster == who])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reconstruction gray values mapping\n",
    "Data['RecGrayValues'] = [get_rec_gray_values(logfile) for logfile in Data.LogFile]\n",
    "Data['Rec Gray Min'] = [v[0] for v in Data['RecGrayValues']]\n",
    "Data['Rec Gray Max'] = [v[1] for v in Data['RecGrayValues']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rec Gray Minimum mean\n",
    "for c, row in Data.iterrows():\n",
    "    if row['Rec Gray Min'] != 0:\n",
    "        print('Please re-reconstruct sample %s with a lower gray value of 0.0' % row.Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rec Gray Maximum\n",
    "Data[['Sample', 'Rec Gray Max']][Data['Rec Gray Max'] == Data['Rec Gray Max'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full head of Control06 was re-reconstructed for visualization purposes, thus it has a differing gray value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Data.Sample, Data['Rec Gray Min'])\n",
    "plt.axhline(Data['Rec Gray Min'].mean(), c=seaborn.color_palette()[0])\n",
    "plt.scatter(Data.Sample, Data['Rec Gray Max'])\n",
    "plt.axhline(Data['Rec Gray Max'].mean(), c=seaborn.color_palette()[1])\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    if row['Rec Gray Min'] != 0:\n",
    "        print('%s has a non-zero minimal reconstruction gray value, namely %0.3f (scanned by %s)' % (row.Sample,\n",
    "                                                                                                     row['Rec Gray Min'],\n",
    "                                                                                                     row.ScanMaster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the voxel volume, which we'll use later on\n",
    "# Let's use microlitre as unit\n",
    "Data['VoxelVolume'] = [ps ** 3 * 1e-9 for ps in Data.PixelSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ourselves a 'fish colum', which we need later on for merging different data frames\n",
    "# (e.g. Gill volume and morphology)\n",
    "Data['Fish'] = [int(n.split('_')[0][-2:]) for n in Data.Sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the other data (XLS files from Matthias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fish length data\n",
    "morphologyfile = glob.glob(os.path.join('XLS*', '*morph*.xlsx'))[0]\n",
    "# Load all the morphology data into its own dataframe\n",
    "# With a hat tip to https://stackoverflow.com/a/49442625/323100\n",
    "Morphology = pandas.read_excel(morphologyfile,\n",
    "                               usecols='E:H,K:N',\n",
    "                               skiprows=3,\n",
    "                               nrows=27,\n",
    "                               names=['Length Swimmer Before',\n",
    "                                      'Weight Swimmer Before',\n",
    "                                      'Length Control Before',\n",
    "                                      'Weight Control Before',\n",
    "                                      'Length Swimmer After',\n",
    "                                      'Weight Swimmer After',\n",
    "                                      'Length Control After',\n",
    "                                      'Weight Control After'])\n",
    "# Drop unneeded rows\n",
    "Morphology.drop([10, 11, 12, 13, 14, 15, 16], inplace=True)\n",
    "# Get ourselves the fish number in a column\n",
    "Morphology['Fish'] = range(1, 21)\n",
    "Morphology['Fish'][10:] = range(1, 11)\n",
    "# Get ourselves the gender in a column\n",
    "Morphology['Gender'] = 'Female'\n",
    "Morphology['Gender'][10:] = 'Male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Massage the morphology data into a new dataframe for displaying it nicely\n",
    "# One fish for the swimmers after training died, so we temporarily\n",
    "# replace it's data with 9999 to we can still do the 'dropna' dance below\n",
    "Morphology.fillna(value=9999, inplace=True)\n",
    "# First, pull together so we get a proper 'Training' column\n",
    "mo = pandas.concat([Morphology[['Gender',\n",
    "                                'Fish',\n",
    "                                'Length Control Before',\n",
    "                                'Length Swimmer Before',\n",
    "                                'Weight Control Before',\n",
    "                                'Weight Swimmer Before']],\n",
    "                    Morphology[['Gender',\n",
    "                                'Fish',\n",
    "                                'Length Control After',\n",
    "                                'Length Swimmer After',\n",
    "                                'Weight Control After',\n",
    "                                'Weight Swimmer After']]])\n",
    "\n",
    "mo['Length Control'] = pandas.concat([mo['Length Control Before'].dropna(),\n",
    "                                      mo['Length Control After'].dropna()])\n",
    "mo['Length Swimmer'] = pandas.concat([mo['Length Swimmer Before'].dropna(),\n",
    "                                      mo['Length Swimmer After'].dropna()])\n",
    "mo['Weight Control'] = pandas.concat([mo['Weight Control Before'].dropna(),\n",
    "                                      mo['Weight Control After'].dropna()])\n",
    "mo['Weight Swimmer'] = pandas.concat([mo['Weight Swimmer Before'].dropna(),\n",
    "                                      mo['Weight Swimmer After'].dropna()])\n",
    "mo['Training'] = ['Before' if o > 0 else 'After' for o in mo['Length Control Before']]\n",
    "# Then, pull together so we get a proper 'Experiment' column\n",
    "mo2 = pandas.concat([mo[['Gender',\n",
    "                         'Fish',\n",
    "                         'Training',\n",
    "                         'Length Control',\n",
    "                         'Weight Control']],\n",
    "                     mo[['Gender',\n",
    "                         'Fish',\n",
    "                         'Training',\n",
    "                         'Length Swimmer',\n",
    "                         'Weight Swimmer']]])\n",
    "mo2['Length'] = pandas.concat([mo2['Length Control'].dropna(),\n",
    "                               mo2['Length Swimmer'].dropna()])\n",
    "mo2['Weight'] = pandas.concat([mo2['Weight Control'].dropna(),\n",
    "                               mo2['Weight Swimmer'].dropna()])\n",
    "mo2['Experiment'] = ['Swimmer' if i > 0 else 'Control' for i in mo2['Length Swimmer']]\n",
    "# Set that one fish back to NaN\n",
    "mo2.replace(9999, numpy.nan, inplace=True)\n",
    "# Set the massaged data back to the Morphology dataframe\n",
    "Morphology = mo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge (correct) morphology data into the other data\n",
    "# We need that later on for normalizing calculated values to the fish length\n",
    "Data = pandas.merge(Data,\n",
    "                    Morphology[(Morphology.Gender == 'Female')\n",
    "                               & (Morphology.Training == 'After')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the respirometry data\n",
    "# Since it's in a 'messy' xls sheet, we cannot do it in one go, but do it in four cells :)\n",
    "respirometryfile = glob.glob(os.path.join('XLS*', '*', 'Respirometry*.xlsx'))[0]\n",
    "# Based on https://stackoverflow.com/a/49442625/323100\n",
    "# Load O2 at start\n",
    "o2 = pandas.read_excel(respirometryfile, index_col=None, skiprows=1, usecols='H')\n",
    "Data['O2 consumption start'] = pandas.concat([o2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load O2 at start, normalized\n",
    "o2_norm = pandas.read_excel(respirometryfile, index_col=None, skiprows=1, usecols='I')\n",
    "# Unfortunately, Matthias saved some more stuff into this colum, so we just drop some cells\n",
    "o2_norm.drop([10, 21, 22], inplace=True)\n",
    "Data['O2 consumption start normalized'] = pandas.concat([o2_norm], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load O2 at the end\n",
    "o2_end = pandas.read_excel(respirometryfile, index_col=None, skiprows=1, usecols='P')\n",
    "Data['O2 consumption end'] = pandas.concat([o2_end], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load O2 at end, normalized\n",
    "o2_norm_end = pandas.read_excel(respirometryfile, index_col=None, skiprows=1, usecols='Q')\n",
    "# Unfortunately, Matthias saved some more stuff into this colum, so we just drop some cells\n",
    "o2_norm_end.drop([10, 20], inplace=True)\n",
    "Data['O2 consumption end normalized'] = pandas.concat([o2_norm_end], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check the data\n",
    "# Data[['Sample',\n",
    "#       'O2 consumption start', 'O2 consumption start normalized',\n",
    "#       'O2 consumption end', 'O2 consumption end normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Massage the respirometry data into a new dataframe for displaying it nicely\n",
    "# On some respirometry data we have NaN.\n",
    "# Let's fill this with 9999, so that we can merge the before/after data\n",
    "Data.fillna(value=9999, inplace=True)\n",
    "# Generate us a new dataframe\n",
    "Respirometry = pandas.concat([Data[['Sample', 'Experiment', 'O2 consumption start normalized']],\n",
    "                              Data[['Sample', 'Experiment', 'O2 consumption end normalized']]])\n",
    "# Let's put NaN back\n",
    "Data.replace(9999, numpy.nan, inplace=True)\n",
    "# Merge, based on https://stackoverflow.com/a/10972557/323100\n",
    "Respirometry['o2'] = pandas.concat([Respirometry['O2 consumption start normalized'].dropna(),\n",
    "                                    Respirometry['O2 consumption end normalized'].dropna()])\n",
    "# Make us an experiment state, so we can 'hue' on this later\n",
    "Respirometry['Training'] = ['Before' if o > 0 else 'After' for o in Respirometry['O2 consumption start normalized']]\n",
    "# Set that one value back to NaN\n",
    "Respirometry.replace(9999, numpy.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SEM data\n",
    "semfile = glob.glob(os.path.join('XLS*', '*', '*electron*.xlsx'))[0]\n",
    "# Load filament count\n",
    "# We could probably do it in one go as above, but then 'squeeze' doesn't seem to discard the empty lines...\n",
    "Filaments = pandas.DataFrame()\n",
    "Filaments['Swimmer Count'] = pandas.read_excel(semfile, usecols='G', nrows=199, squeeze=True)\n",
    "Filaments['Control Count'] = pandas.read_excel(semfile, usecols='AA', nrows=199, squeeze=True, na_values='Keine Daten')\n",
    "# Load filament length\n",
    "Filaments['Swimmer Length'] = pandas.read_excel(semfile, usecols='Q', nrows=199, squeeze=True)\n",
    "Filaments['Control Length'] = pandas.read_excel(semfile, usecols='AK', nrows=199, squeeze=True, na_values='Keine Daten')\n",
    "# Scale from Fiji measurement units to um\n",
    "Filaments['Swimmer Length'] *= 1 / 0.6 * 100\n",
    "Filaments['Control Length'] *= 1 / 0.6 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the speed assessment data\n",
    "speedfile = glob.glob(os.path.join('XLS*', '*speed*.xlsx'))[0]\n",
    "# Load two empty colums (G, H) for dummy data ('Control 3wk' and 'Control rpm 3wk')\n",
    "Speed = pandas.read_excel(speedfile,\n",
    "                          usecols='B,C,E,F,G,H,O,P,R,S,U,V',\n",
    "                          skiprows=3,\n",
    "                          nrows=29,\n",
    "                          names=['Swimmer Before',\n",
    "                                 'Swimmer rpm Before',\n",
    "                                 'Control Before',\n",
    "                                 'Control rpm Before',\n",
    "                                 'Control 3wk',\n",
    "                                 'Control rpm 3wk',\n",
    "                                 'Swimmer 3wk',\n",
    "                                 'Swimmer rpm 3wk',\n",
    "                                 'Swimmer After',\n",
    "                                 'Swimmer rpm After',\n",
    "                                 'Control After',\n",
    "                                 'Control rpm After'])\n",
    "# Drop unneeded colums and reset the numbering (index)\n",
    "Speed.drop([10, 11, 12, 13, 14, 15, 16, 17, 18], inplace=True)\n",
    "Speed.reset_index(inplace=True, drop=True)\n",
    "# Convert the weird Excel time to something we can actually use...\n",
    "# Pandas parses the M:S Matthias entered as H:M:00\n",
    "# It would probably easy to do this via a parser, but just converting it with a Timedelta is quicker\n",
    "# (Which just means I gave up after putting keywords into a search engine for two hours\n",
    "Speed['Swimmer Before'] = [pandas.Timedelta(minutes=time.hour,\n",
    "                                            seconds=time.minute) for time in Speed['Swimmer Before']]\n",
    "Speed['Control Before'] = [pandas.Timedelta(minutes=time.hour,\n",
    "                                            seconds=time.minute) for time in Speed['Control Before']]\n",
    "# Since we have NaN (as float) in the column (data for fish 10 is missing)\n",
    "# we jump through this super-complicated hoop to covert the time to timedelta so that we have NaT\n",
    "# with a tip to the hat to https://stackoverflow.com/a/25142407/323100\n",
    "Speed['Swimmer 3wk'] = [pandas.Timedelta(minutes=time.hour, seconds=time.minute)\n",
    "                        if not pandas.isnull(time) else pandas.to_datetime('13000101',\n",
    "                                                                           format='%Y%m%d',\n",
    "                                                                           errors='coerce')\n",
    "                        for time in Speed['Swimmer 3wk']]\n",
    "Speed['Swimmer After'] = [pandas.Timedelta(minutes=time.hour, seconds=time.minute)\n",
    "                          if not pandas.isnull(time) else pandas.to_datetime('13000101',\n",
    "                                                                             format='%Y%m%d',\n",
    "                                                                             errors='coerce')\n",
    "                          for time in Speed['Swimmer After']]\n",
    "Speed['Control After'] = [pandas.Timedelta(minutes=time.hour,\n",
    "                                           seconds=time.minute) for time in Speed['Control After']]\n",
    "# Get ourselves the fish number in a column\n",
    "Speed['Fish'] = range(1, 21)\n",
    "Speed['Fish'][10:] = range(1, 11)\n",
    "# Get ourselves the gender in a column\n",
    "Speed['Gender'] = 'Female'\n",
    "Speed['Gender'][10:] = 'Male'\n",
    "# Fill the dummy colums with dummy values\n",
    "Speed['Control 3wk'] = pandas.Timedelta(minutes=99)\n",
    "Speed['Control rpm 3wk'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Massage the speed data into a new dataframe for displaying it nicely\n",
    "# One fish for the swimmers after training died missing,\n",
    "# replace it's data with 9999 so we can still do the 'dropna' dance below\n",
    "Speed.fillna(value=9999, inplace=True)\n",
    "# First, pull together so we get a proper 'Training' column\n",
    "sp = pandas.concat([Speed[['Gender',\n",
    "                           'Fish',\n",
    "                           'Control Before',\n",
    "                           'Swimmer Before']],\n",
    "                    Speed[['Gender',\n",
    "                           'Fish',\n",
    "                           'Control 3wk',\n",
    "                           'Swimmer 3wk']],\n",
    "                    Speed[['Gender',\n",
    "                           'Fish',\n",
    "                           'Control After',\n",
    "                           'Swimmer After']]])\n",
    "sp['Control'] = pandas.concat([sp['Control Before'].dropna(),\n",
    "                               sp['Control 3wk'].dropna(),\n",
    "                               sp['Control After'].dropna()])\n",
    "sp['Swimmer'] = pandas.concat([sp['Swimmer Before'].dropna(),\n",
    "                               sp['Swimmer 3wk'].dropna(),\n",
    "                               sp['Swimmer After'].dropna()])\n",
    "# Make us a 'Training' column\n",
    "sp['Training'] = 'Before'\n",
    "# https://stackoverflow.com/a/12307162/323100\n",
    "sp.loc[sp['Swimmer 3wk'] > pandas.Timedelta(seconds=1), 'Training'] = '3 wk'\n",
    "sp.loc[sp['Swimmer After'] > pandas.Timedelta(seconds=1), 'Training'] = '5 wk'\n",
    "# Then, pull together so we get a proper 'Experiment' column\n",
    "sp2 = pandas.concat([sp[['Gender',\n",
    "                         'Fish',\n",
    "                         'Training',\n",
    "                         'Control']],\n",
    "                     sp[['Gender',\n",
    "                         'Fish',\n",
    "                         'Training',\n",
    "                         'Swimmer']]])\n",
    "sp2['Time'] = pandas.concat([sp2['Control'].dropna(),\n",
    "                             sp2['Swimmer'].dropna()])\n",
    "sp2['Experiment'] = ['Swimmer' if i > pandas.Timedelta(seconds=0) else 'Control' for i in sp2['Swimmer']]\n",
    "# Set the special values back to NaT/NaN\n",
    "sp2.replace(pandas.Timedelta(seconds=9999.), numpy.nan, inplace=True)\n",
    "sp2.replace(pandas.Timedelta(seconds=5940.), numpy.nan, inplace=True)\n",
    "# Set the massaged data back to the Morphology dataframe\n",
    "Speed = sp2\n",
    "# Make us a 'total seconds' column for plotting later on\n",
    "Speed['Seconds'] = [t.total_seconds() for t in Speed.Time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now that we have read all the data we need, we actually read the images.\n",
    "And do some image processing to get to the gill volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Overview images\n",
    "Overviews = [imageio.imread(o) for o in Data.OverviewName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display overviews (shadow projections)\n",
    "for c, o in enumerate(Overviews):\n",
    "    plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "    plt.imshow(o)\n",
    "    plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um', color='black'))\n",
    "    plt.axis('off')\n",
    "    plt.title('%s' % Data.Sample[c])\n",
    "plt.suptitle('Overviews')\n",
    "plt.savefig(os.path.join(OutPutDir, 'Overviews.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all VOI slices of each fish into single NumPy arrays and save them to disk\n",
    "# But only do this if we didn't do it already :)\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# Since we reload/memorymap the stack below, we overwrite the variable with NaN and clear the memory\n",
    "Data['OutputNameVOI'] = [os.path.join(f, sample + '_VOI.npy') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "# Don't save into the dataframe, or else we won't be able to make it :)\n",
    "VOIImages = [numpy.nan for file in Data.OutputNameVOI]\n",
    "for c, voi in enumerate(Data.OutputNameVOI):\n",
    "    # Only do this if we didn't do it already...\n",
    "    if os.path.exists(voi):\n",
    "        print('%2s/%s: %16s: Already saved to %s' % (c + 1, len(Data.Sample), Data.Sample[c], voi[len(RootFolder):]))\n",
    "    else:\n",
    "        print('%2s/%s: %16s: Reading %4s VOI images' % (c + 1, len(Data.Sample), Data.Sample[c], len(Data.VOINames[c])))\n",
    "        # Actually load the images now\n",
    "        VOIImages[c] = numpy.array([scipy.misc.imread(i, flatten=True) for i in Data.VOINames[c]])\n",
    "        # Save the images to NumPy binary files, disallowing pickle for portability\n",
    "        print('%23s: Saving to %s' % (Data.Sample[c], voi))\n",
    "        numpy.save(voi, VOIImages[c], allow_pickle=False)\n",
    "        # Clear memory\n",
    "        VOIImages[c] = numpy.nan\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the memory of the images we loaded.\n",
    "# We 'memory-map' them again below\n",
    "%xdel VOIImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files.\n",
    "# This is loading the images like a virtual stack in ImageJ\n",
    "VOIImages = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ourselves the middle slice to show\n",
    "Data['MiddleSliceName'] = [n[len(n) // 2] for n in Data.VOINames]\n",
    "Data['MiddleSlice'] = [i[len(i) // 2] for i in VOIImages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the middle slice of the VOI\n",
    "for c, m in enumerate(Data.MiddleSlice):\n",
    "    plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "    plt.imshow(m, vmax=0.618 * numpy.max(m))\n",
    "    plt.title('%s | VOI Slice %s' % (Data.Sample[c], os.path.basename(Data.MiddleSliceName[c])[-8:-4]))\n",
    "    plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Middle slices')\n",
    "# plt.savefig(os.path.join(OutPutDir, 'MiddleSlices.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ourselves some information on the VOI size\n",
    "Data['NumberOfVOISlices'] = [len(v) for v in Data.VOINames]\n",
    "Data['VOIShape'] = [voi.shape for voi in VOIImages]\n",
    "Data['VOIVolume'] = [shape[0] * shape[1] * shape[2] for shape in Data.VOIShape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of VOI size and number of files is consistent.\n",
    "# If there's no output from this cell, then everything is correct.\n",
    "for c, row in Data.iterrows():\n",
    "    if len(row.VOINames) != len(VOIImages[c]):\n",
    "        print(row.Sample)\n",
    "        print(len(row.VOINames))\n",
    "        print(row.VOIShape[0])\n",
    "        print(len(VOIImages[c]))\n",
    "        print(80 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average volume/size\n",
    "print('The average VOI site is a cube '\n",
    "      'with a side length of %s pixels' % int(round(Data.VOIVolume.mean() ** (1 / 3.))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bragging reasons in the manuscript\n",
    "# 'sort_values' does an ascending sort by default, so let's look at the biggest (last/'tail(1)') item\n",
    "print('The largest VOI is')\n",
    "print(Data[['Sample', 'VOIVolume', 'NumberOfVOISlices', 'VOIShape']].sort_values('VOIVolume').tail(1))\n",
    "print(80 * '-')\n",
    "# 'sort_values' does an ascending sort by default, so let's look at the smallest (first/'head(1)') item\n",
    "print('The smallest VOI is')\n",
    "print(Data[['Sample', 'VOIVolume', 'NumberOfVOISlices', 'VOIShape']].sort_values('VOIVolume').head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a manageable amount of equally spaced slices for thresholding and MIP-ing\n",
    "# This was done in the evaluation stage of the notebook for speed-up reasons\n",
    "# NumberOfImagesToShow = 15\n",
    "# NumberOfImagesToShow = 111\n",
    "# NumberOfImagesToShow = 350\n",
    "# NumberOfImagesToShow = 1111\n",
    "# For the finalized version, we just use *all* the slices we have\n",
    "NumberOfImagesToShow = Data.NumberOfVOISlices.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the subset information\n",
    "Data['PrintEverySlice'] = [int(round(len(r) / NumberOfImagesToShow)) for r in Data.VOINames]\n",
    "Data['SubsetNames'] = [rn[::sw] for rn, sw in zip(Data.VOINames, Data.PrintEverySlice)]\n",
    "for c, i in enumerate(Data.Sample):\n",
    "    print('For %s we are working with a subset of %s (%0.1f %% of totally %s) equally '\n",
    "          'spaced slices' % (i,\n",
    "                             len(Data.SubsetNames[c]),\n",
    "                             len(Data.SubsetNames[c]) / len(Data.VOINames[c]) * 100,\n",
    "                             len(Data.VOINames[c])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put length of 'UseThis' into dataframe\n",
    "Data['NumberOfAnalyzedVOISlices'] = [len((a)) for a in Data.SubsetNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the thresholds on each and every image\n",
    "Data['OutputNameThreshold'] = [os.path.join(f,\n",
    "                                            sample + '_thresholds_from_%04d_of_%04d_slices.npy' % (len(n[::p]),\n",
    "                                                                                                   len(n)))\n",
    "                               for f, sample, n, p in zip(Data.Folder,\n",
    "                                                          Data.Sample,\n",
    "                                                          Data.VOINames,\n",
    "                                                          Data.PrintEverySlice)]\n",
    "Data['Threshold'] = [[numpy.nan] for s in Data.Sample]\n",
    "for c, thresholdname in enumerate(Data.OutputNameThreshold):\n",
    "    if os.path.exists(thresholdname):\n",
    "        print('%2s/%s: %16s: Grab values from %s' % (c + 1,\n",
    "                                                     len(Data),\n",
    "                                                     Data.Sample[c],\n",
    "                                                     thresholdname[len(RootFolder):]))\n",
    "        Data['Threshold'][c] = numpy.load(thresholdname)\n",
    "    else:\n",
    "        print('%2s/%s: %16s: Calculating thresholds for %s of %4s files' % (c + 1,\n",
    "                                                                            len(Data),\n",
    "                                                                            Data.Sample[c],\n",
    "                                                                            len(Data.VOINames[c][::Data.PrintEverySlice[c]]),\n",
    "                                                                            len(Data.VOINames[c])))\n",
    "        Data['Threshold'][c] = [numpy.nan] * len(Data.VOINames[c][::Data.PrintEverySlice[c]])\n",
    "        for d, image in tqdm_notebook(enumerate(VOIImages[c][::Data.PrintEverySlice[c]]),\n",
    "                                      total=len(Data.VOINames[c]),\n",
    "                                      leave=False):\n",
    "            try:\n",
    "                # Calculate and save threshold (of only the image, e.g. img[img>0])\n",
    "                Data['Threshold'][c][d] = skimage.filters.threshold_otsu(image[image > 0])\n",
    "            except (ValueError):\n",
    "                # Save NaN if we can't calculate a threshold\n",
    "                Data['Threshold'][c][d] = numpy.nan\n",
    "        print('%23s: Saving thresholds to %s' % (Data.Sample[c], thresholdname[len(RootFolder):]))\n",
    "        numpy.save(thresholdname, Data.Threshold[c], allow_pickle=False)\n",
    "        Data.Threshold[c] = numpy.nan\n",
    "        # Unmap current VOIImage: https://stackoverflow.com/a/6398543\n",
    "        VOIImages[c]._mmap.close()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the VOI images again, since we closed them above\n",
    "# This is loading the images like a virtual stack in ImageJ\n",
    "VOIImages = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files.\n",
    "# This is loading the images like a virtual stack in ImageJ\n",
    "Data['Threshold'] = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameThreshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the max, mean and median threshold for each sample\n",
    "Data['ThresholdMax'] = [numpy.nanmax(t) for t in Data.Threshold]\n",
    "# Discard the first and last $discard slices for the averaging\n",
    "discard = 150\n",
    "Data['ThresholdAverage'] = [numpy.nanmean(t[discard:-discard]) for t in Data.Threshold]\n",
    "Data['ThresholdMedian'] = [numpy.nanmedian(t) for t in Data.Threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Thresholds\n",
    "for c, i in enumerate(Data.Threshold):\n",
    "    plt.subplot(1, len(Data), c + 1)\n",
    "    seaborn.violinplot(i[discard:-discard], orient='v', color=Data.Color[c], cut=0, width=0.5)\n",
    "    plt.axhline(Data.ThresholdAverage[c], label='Average', color=Data.Color[c])\n",
    "    plt.axhline(Data.ThresholdMedian[c], label='Median', color=Data.Color[c], ls='dashed')\n",
    "    plt.ylim([0, Data.ThresholdMax.max() * 1.1])\n",
    "    if c:\n",
    "        plt.gca().axes.yaxis.set_ticklabels([])\n",
    "    else:\n",
    "        plt.ylabel('Threshold')\n",
    "    if '05' in Data.Sample[c]:\n",
    "        plt.legend()\n",
    "    plt.xlabel(Data.Sample[c], rotation=90)\n",
    "plt.suptitle('Otsu Thresholds for ~%s VOI slices' % NumberOfImagesToShow)\n",
    "# plt.savefig(os.path.join(OutPutDir, 'Thresholds_from%04dslices.png' % NumberOfImagesToShow), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot all the single thresholds\n",
    "for c, i in enumerate(Data.Threshold):\n",
    "    plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "    plt.plot(i[discard:-discard], color=Data.Color[c])\n",
    "    plt.plot(sorted(i[discard:-discard]), color=Data.Color[c], alpha=0.618)\n",
    "    plt.axhline(Data.ThresholdAverage[c], label='Average', color=Data.Color[c])\n",
    "    plt.axhline(Data.ThresholdMedian[c], label='Median', color=Data.Color[c], ls='dashed')\n",
    "    plt.ylim([0, Data.ThresholdMax.max() * 1.1])\n",
    "    plt.title(Data.Sample[c])\n",
    "    plt.gca().axes.xaxis.set_ticklabels([])\n",
    "    plt.gca().axes.yaxis.set_ticklabels([])\n",
    "    if '01' in Data.Sample[c]:\n",
    "        plt.legend()\n",
    "plt.suptitle('Otsu Thresholds for ~%s VOI slices' % NumberOfImagesToShow)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(OutPutDir, 'Thresholds-plot_from%04dslices.png' % NumberOfImagesToShow), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we should use the average or median\n",
    "Data['ThresholdDifference'] = [numpy.subtract(a,m) for a,m in zip(Data.ThresholdAverage,\n",
    "                                                                  Data.ThresholdMedian)]\n",
    "Data[['Sample', 'ThresholdAverage', 'ThresholdMedian', 'ThresholdDifference']]\n",
    "seaborn.boxplot(Data['ThresholdDifference'])\n",
    "# Average and median are nearly the same, so let's keep using the average\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a subset of images\n",
    "Data['OutputNameVOISubset'] = [os.path.join(f,\n",
    "                                            sample + '_VOI_subset_from_%04d_of_%04d_slices.npy' % (len(n[::p]),\n",
    "                                                                                                   len(n)))\n",
    "                               for f, sample, n, p in zip(Data.Folder,\n",
    "                                                          Data.Sample,\n",
    "                                                          Data.VOINames,\n",
    "                                                          Data.PrintEverySlice)]\n",
    "# Don't save into the dataframe, or else we won't be able to make it to the end :)\n",
    "VOISubset = [numpy.nan for file in Data.OutputNameVOISubset]\n",
    "for c, subset in enumerate(Data.OutputNameVOISubset):\n",
    "    if len(Data.VOINames[c]) != len(Data.VOINames[c][::Data.PrintEverySlice[c]]):\n",
    "        # We are using a subset\n",
    "        # Only do this if we didn't do it already...\n",
    "        if os.path.exists(subset):\n",
    "            print('%2s/%s: %16s: Already saved to %s' % (c + 1,\n",
    "                                                         len(Data.Sample),\n",
    "                                                         Data.Sample[c],\n",
    "                                                         subset[len(RootFolder):]))\n",
    "        else:\n",
    "            print('%2s/%s: %16s: Subsetting %s of %s VOI images' % (c + 1,\n",
    "                                                                    len(Data.Sample),\n",
    "                                                                    Data.Sample[c],\n",
    "                                                                    len(Data.SubsetNames[c]),\n",
    "                                                                    len(Data.VOINames[c])))\n",
    "            VOISubset[c] = [numpy.nan] * len(Data.VOINames[c][::Data.PrintEverySlice[c]])\n",
    "            for d, image in enumerate(VOIImages[c][::Data.PrintEverySlice[c]]):\n",
    "                VOISubset[c][d] = image\n",
    "            print('%23s: Saving subset to %s' % (Data.Sample[c],\n",
    "                                                 subset[len(RootFolder):]))\n",
    "            numpy.save(subset, VOISubset[c], allow_pickle=False)\n",
    "            VOISubset[c] = numpy.nan\n",
    "            gc.collect()\n",
    "    else:\n",
    "        # We are using the full dataset\n",
    "        print('%2s/%s: %16s: Using the full dataset' % (c + 1,\n",
    "                                                        len(Data.Sample),\n",
    "                                                        Data.Sample[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory\n",
    "%xdel VOISubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files.\n",
    "if Data.PrintEverySlice[0] == 1:\n",
    "    # If we did NOT use a subset, load the original stack...\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOI]\n",
    "else:\n",
    "    # If we did use a subset, then load the subset\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOISubset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the longest samplename, for display reasons below\n",
    "# Based on https://stackoverflow.com/a/21295630/323100\n",
    "namelenmax = Data.Sample.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one threshold for all images (UseAverageThreshold=True, ThresholdAverage)\n",
    "# Or use the Otsu threshold for each and every image (UseAverageThreshold=False)\n",
    "UseAverageThreshold = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually threshold the images\n",
    "if UseAverageThreshold:\n",
    "    Data['OutputNameThresholded'] = [os.path.join(f,\n",
    "                                                  sample + '_gills_thresholded-with-%0.2f.npy' % ta)\n",
    "                                     for f, sample, ta in zip(Data.Folder,\n",
    "                                                              Data.Sample,\n",
    "                                                              Data.ThresholdAverage)]\n",
    "else:\n",
    "    Data['OutputNameThresholded'] = [os.path.join(f,\n",
    "                                                  sample + '_gills_thresholded-slicewise.npy')\n",
    "                                     for f, sample in zip(Data.Folder,\n",
    "                                                          Data.Sample)]\n",
    "Thresholded = [numpy.nan for file in Data.OutputNameThreshold]\n",
    "for c, sample in Data.iterrows():\n",
    "    # Only do this if we didn't do it already...\n",
    "    if os.path.exists(sample.OutputNameThresholded):\n",
    "        print('%2s/%s: %s: Already saved to %s' % (c + 1,\n",
    "                                                   len(Data.Sample),\n",
    "                                                   sample.Sample.rjust(namelenmax),\n",
    "                                                   os.path.basename(sample.OutputNameThresholded)))\n",
    "    else:\n",
    "        if UseAverageThreshold:\n",
    "            print('%2s/%s: %s: Thresholding %3s VOI images with %0.2f' % (c + 1,\n",
    "                                                                          len(Data.Sample),\n",
    "                                                                          sample.Sample.rjust(namelenmax),\n",
    "                                                                          len(sample.VOINames),\n",
    "                                                                          sample.ThresholdAverage))\n",
    "        else:\n",
    "            print('%2s/%s: %s: Thresholding %3s VOI images slicewise' % (c + 1,\n",
    "                                                                         len(Data.Sample),\n",
    "                                                                         sample.Sample.rjust(namelenmax),\n",
    "                                                                         len(sample.VOINames)))\n",
    "        Thresholded[c] = [None] * len(sample.VOINames)\n",
    "        for d, image in tqdm_notebook(enumerate(VOISubset[c]),\n",
    "                                      total=len(VOISubset[c]),\n",
    "                                      leave=False):\n",
    "            if UseAverageThreshold:\n",
    "                Thresholded[c][d] = image > sample.Threshold[d]\n",
    "            else:\n",
    "                Thresholded[c][d] = image > sample.ThresholdAverage\n",
    "        # Save the images to NumPy binary files, disallowing pickle for portability\n",
    "        print('%s: Saving to %s' % (os.path.basename(sample.Sample.rjust(namelenmax + 7)),\n",
    "                                    sample.OutputNameThresholded[len(RootFolder):]))\n",
    "        numpy.save(sample.OutputNameThresholded, Thresholded[c], allow_pickle=False)\n",
    "        Thresholded[c] = numpy.nan\n",
    "        # Unmap current VOISubset: https://stackoverflow.com/a/6398543\n",
    "        VOISubset[c]._mmap.close()\n",
    "        gc.collect()\n",
    "if UseAverageThreshold:\n",
    "    print('\\n\\nWe were using a single threshold (ThresholdAverage) for *all* the images')\n",
    "else:\n",
    "    print('\\n\\nWe were using the Otsu threshold for each and every single image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files again.\n",
    "if Data.PrintEverySlice[0] == 1:\n",
    "    # If we did NOT use a subset, load the original stack...\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOI]\n",
    "else:\n",
    "    # If we did use a subset, then load the subset\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOISubset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files again.\n",
    "Thresholded = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameThresholded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display middle slices and thresholded equivalent\n",
    "# for c, fish in Data.iterrows():\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(VOIImages[c][len(VOIImages[c]) // 2])\n",
    "#     plt.title('%s: Middle slice (...%s)' % (fish.Sample, Data.VOINames[c][len(VOIImages[c]) // 2][-12:]))\n",
    "#     plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(Thresholded[c][len(Thresholded[c]) // 2])\n",
    "#     plt.title('Thresholded with %0.2f' % Data.ThresholdAverage[c])\n",
    "#     plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(os.path.join(OutPutDir, 'MiddleSlice.%s.png' % fish.Sample), bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save thresholded images out\n",
    "SaveImages = True\n",
    "if SaveImages:\n",
    "    # Save thresholded images\n",
    "    for c, folder in enumerate(Data.VOIFolder):\n",
    "        CurrentOutputFolder = os.path.splitext(Data.OutputNameThresholded[c])[0].replace(Data.Sample[c] + '_gills', 'VOI_gills')\n",
    "        if not os.path.exists(CurrentOutputFolder):\n",
    "            os.makedirs(CurrentOutputFolder)\n",
    "        if len(glob.glob(os.path.join(CurrentOutputFolder, '*.png'))) >= len(VOIImages[c]):\n",
    "            print('%2s/%s: %s: Already saved %s thresholded images to %s' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             Data.Sample[c].rjust(namelenmax),\n",
    "                                                                             len(VOIImages[c]),\n",
    "                                                                             CurrentOutputFolder[len(RootFolder) + 1:]))\n",
    "        else:\n",
    "            print('%2s/%s: %s: Saving %s thresholded images to %s' % (c + 1,\n",
    "                                                                      len(Data),\n",
    "                                                                      Data.Sample[c].rjust(namelenmax),\n",
    "                                                                      len(Thresholded[c]),\n",
    "                                                                      CurrentOutputFolder[len(RootFolder) + 1:]))\n",
    "            for d, i in tqdm_notebook(enumerate(Thresholded[c]),\n",
    "                                      total=len(Thresholded[c]),\n",
    "                                      leave=False):\n",
    "                scipy.misc.imsave(os.path.join(CurrentOutputFolder,\n",
    "                                               Data.Sample[c] + '_thresholded_%04d.png' % d), i.astype('uint8') * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Display some original slices through the VOIs\n",
    "# SlicesToShow = 5\n",
    "# Iterator = [int(round(len(r) / SlicesToShow)) for r in Data.VOINames]\n",
    "# for c, sample in Data.iterrows():\n",
    "#     for i in range(SlicesToShow):\n",
    "#         plt.subplot(1,SlicesToShow,i+1)\n",
    "#         plt.imshow(VOIImages[c][::Iterator[c]][i])\n",
    "#         plt.axis('off')\n",
    "#         if i:\n",
    "#             plt.title('Slice %s' % i)\n",
    "#         else:\n",
    "#             plt.gca().add_artist(ScaleBar(sample.PixelSize, 'um', color='white'))\n",
    "#             plt.title(sample.Sample)\n",
    "#     plt.savefig(os.path.join(OutPutDir, 'Sampler_%s_%s-images_originals.png' % (sample.Sample, SlicesToShow)), bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Display some thresholded slices through the VOIs\n",
    "# for c, sample in Data.iterrows():\n",
    "#     for i in range(SlicesToShow):\n",
    "#         plt.subplot(1,SlicesToShow,i+1)\n",
    "#         plt.imshow(Thresholded[c][::Iterator[c]][i])\n",
    "#         plt.axis('off')\n",
    "#         if i:\n",
    "#             plt.title('Slice %s' % i)\n",
    "#         else:\n",
    "#             plt.gca().add_artist(ScaleBar(sample.PixelSize, 'um', color='white'))\n",
    "#             plt.title(sample.Sample)\n",
    "#     plt.savefig(os.path.join(OutPutDir, 'Sampler_%s_%s-images_thresholded.png' % (sample.Sample, SlicesToShow)), bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or generate the MIPs\n",
    "Data['OutputNameMIP'] = [os.path.join(f,\n",
    "                                      sample + '_MIP_from_%04d_of_%04d_slices.npy' % (len(n[::p]),\n",
    "                                                                                      len(n)))\n",
    "                         for f, sample, n, p in zip(Data.Folder,\n",
    "                                                    Data.Sample,\n",
    "                                                    Data.VOINames,\n",
    "                                                    Data.PrintEverySlice)]\n",
    "MIPs = [None] * len(Data)\n",
    "for c, fn in enumerate(Data.OutputNameMIP):\n",
    "    if os.path.exists(fn):\n",
    "        print('%2s/%s: %16s: Loading %s into memory' % (c + 1,\n",
    "                                                        len(Data),\n",
    "                                                        Data.Sample[c],\n",
    "                                                        fn[len(RootFolder):]))\n",
    "        MIPs[c] = numpy.load(fn, mmap_mode='r')\n",
    "    else:\n",
    "        print('%2s/%s: %16s: Generating MIP from %s images' % (c + 1,\n",
    "                                                               len(Data),\n",
    "                                                               Data.Sample[c],\n",
    "                                                               len(Data.SubsetNames[c])))\n",
    "        MIPs[c] = numpy.max(VOISubset[c], axis=0)\n",
    "        numpy.save(fn, MIPs[c], allow_pickle=False)\n",
    "        # Free up memory\n",
    "        VOISubset[c]._mmap.close()\n",
    "        gc.collect()\n",
    "    # Save MIP to PNG image\n",
    "    scipy.misc.imsave(os.path.splitext(fn)[0] + '.png', MIPs[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files again, since we closed them above.\n",
    "if Data.PrintEverySlice[0] == 1:\n",
    "    # If we did NOT use a subset, load the original stack...\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOI]\n",
    "else:\n",
    "    # If we did use a subset, then load the subset\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOISubset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display x,y,z-MIPs\n",
    "# for c, sample in enumerate(VOISubset):\n",
    "#     for ax in range(3):\n",
    "#         plt.subplot(1,3,ax+1)\n",
    "#         plt.imshow(numpy.max(sample, axis=ax))\n",
    "#         plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "#         plt.title('%s MIP axis %s' % (Data.Sample[c], ax))\n",
    "#         plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display calculated MIPs\n",
    "for c, m in enumerate(MIPs):\n",
    "    plt.subplot(lines, numpy.ceil(len(SampleNames) / float(lines)), c + 1)\n",
    "    plt.imshow(m)\n",
    "    plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "    plt.title('%s (%s slices)' % (SampleNames[c], len(Data.SubsetNames[c])))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('MIPs')\n",
    "# plt.savefig(os.path.join(OutPutDir, 'MIPs_from%04dslices.png' % NumberOfImagesToShow), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the thresholds in the 60-80% range of the respective values\n",
    "split = 5\n",
    "Selected = [sorted(t)[(split - 2) * len(t) // split:(split - 1) * len(t) // split] for t in Data.Threshold]\n",
    "# mask the values of the threshold that are *not* in this range\n",
    "Otsu_selected = [numpy.ma.masked_outside(o, numpy.min(sel), numpy.max(sel)) for o, sel in zip(Data.Threshold, Selected)]\n",
    "# use the mean of this 60-80% value to threshold the datasets\n",
    "Data['Threshold6080'] = [numpy.nanmean(os) for os in Otsu_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the different thresholds\n",
    "# for c, s in enumerate(SampleNames):\n",
    "#     plt.plot(Data.Threshold[c], marker='', label='%s | Global Otsu Mean=%.2f | 60-80%% Otsu mean=%0.2f' % (s,\n",
    "#                                                                                                            Data.ThresholdAverage[c],\n",
    "#                                                                                                            Data.Threshold6080[c]),\n",
    "#              c=seaborn.color_palette(n_colors=len(SampleNames))[c])\n",
    "#     plt.plot(Otsu_selected[c], '.', ms=5, alpha=0.618,\n",
    "#              c=seaborn.color_palette(n_colors=len(Data))[c])\n",
    "# #     plt.legend(loc='best')\n",
    "# plt.xlim([0, Data.NumberOfVOISlices.max()])\n",
    "# plt.title('Otsu thresholds for each slice of each sample')\n",
    "# plt.savefig(os.path.join(OutPutDir, 'Thresholds-all.png'), bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare global Otsu threshold, 60%-80% selected threshold and threshold from MIPs\n",
    "# for c, s in enumerate(SampleNames):\n",
    "#     print('For %s we have a' % s)\n",
    "#     print('\\t- 60-80%% selected threshold of %0.2f' % Data.Threshold6080[c])\n",
    "#     print('\\t- global Otsu threshold of %0.2f' % Data.ThresholdAverage[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all (global) histograms\n",
    "if not True:\n",
    "    # This takes a while, since we 'ravel' all images from the subsets and calculate their histogram...\n",
    "    for c, s in enumerate(SampleNames):\n",
    "        plt.subplot(lines, numpy.ceil(len(SampleNames) / float(lines)), c + 1)\n",
    "        plt.hist(numpy.array(VOISubset[c]).ravel(),\n",
    "                 bins=32,\n",
    "                 log=True,\n",
    "                 color=Data.Color[c],\n",
    "                 label='%s | %s VOI slices' % (s, len(VOISubset[c])))\n",
    "        if CalculateAllThresholds:\n",
    "            plt.axvline(Data.Threshold6080[c],\n",
    "                        label='Selected Threshold: %0.2f' % Data.Threshold6080[c],\n",
    "                        c=seaborn.color_palette()[2])\n",
    "            plt.axvline(GlobalOtsu[c], label='Mean Otsu Threshold: %0.2f' % GlobalOtsu[c],\n",
    "                        c=seaborn.color_palette()[3])\n",
    "        plt.axvline(Otsu_MIP[c], label='MIP Otsu Threshold: %0.2f' % Otsu_MIP[c],\n",
    "                    c=seaborn.color_palette()[4])\n",
    "        plt.legend()\n",
    "        plt.xlim([0, 255])\n",
    "    plt.savefig(os.path.join(OutPutDir, 'Histograms_Thresholds_from%04dslices.png' % NumberOfImagesToShow), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the different thresholds\n",
    "plt.scatter(x=Data.Sample, y=Data.ThresholdAverage, label='Average Threshold', c=Data.Color, marker='x')\n",
    "plt.scatter(x=Data.Sample, y=Data.ThresholdMedian, label='Median Threshold', c=Data.Color, marker='d')\n",
    "plt.scatter(x=Data.Sample, y=Data.Threshold6080, label='60-80% lThreshold', c=Data.Color)\n",
    "plt.legend()\n",
    "plt.ylim([0, Data.ThresholdAverage.max() * 1.1])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Thresholds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Show middle images with some data\n",
    "# plt.rcParams['figure.figsize'] = (16, 5)\n",
    "# for c, s in enumerate(SampleNames):\n",
    "#     maximum = skimage.filters.rank.maximum(Data.MiddleSlice[c].astype('uint8'), skimage.morphology.disk(25))\n",
    "#     plt.subplot(141)\n",
    "#     plt.imshow(Data.MiddleSlice[c])\n",
    "#     plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "#     plt.title('Middle slice of %s' % s)\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(142)\n",
    "#     plt.imshow(Data.MiddleSlice[c], vmax=0.618 * numpy.max(Data.MiddleSlice[c]))\n",
    "#     plt.imshow(maximum, cmap='viridis', alpha=0.5)\n",
    "#     plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "#     plt.title('%s\\nwith local maxima' % os.path.basename(Data.MiddleSliceName[c]))\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(143)\n",
    "#     plt.hist(Data.MiddleSlice[c].ravel(), bins=32, log=True, color=Data.Color[c])\n",
    "#     plt.axvline(Data.Threshold6080[c],\n",
    "#                 label='60-80%% Threshold: %0.2f' % Data.Threshold6080[c],\n",
    "#                 c=seaborn.color_palette()[2])\n",
    "#     plt.axvline(Data.ThresholdAverage[c], label='Threshold Average: %0.2f' % Data.ThresholdAverage[c],\n",
    "#                 c=seaborn.color_palette()[3])\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.xlim([0, 255])\n",
    "#     plt.title('Histogram of middle slice')\n",
    "#     plt.subplot(144)\n",
    "#     plt.imshow(MIPs[c])\n",
    "#     plt.title('MIP of %s slices of %s' % (len(Data.SubsetNames[c]),\n",
    "#                                           Data.Sample[c]))\n",
    "#     plt.gca().add_artist(ScaleBar(Data.PixelSize[c], 'um'))\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(os.path.join(OutPutDir, 'Details_%s.png' % Data.Sample[c]), bbox_inches='tight')\n",
    "#     plt.show()\n",
    "# plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area_gills(image):\n",
    "    \"\"\"\n",
    "    This function calculates the gill area on an image.\n",
    "    It is simply summing up all the thresholded pixels.\n",
    "    I know we would not need a function for this, but let's try to keep the code clean :)\n",
    "    \"\"\"\n",
    "    area_gills = numpy.sum(image)\n",
    "    return(area_gills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the gill area (pythonic way, but without saving it in the middle...)\n",
    "# Data['AreaGills'] = [[calculate_area_gills(i, threshold=t)\n",
    "#                       for i, t in zip(subset, thrs)]\n",
    "#                      for subset, thrs in zip(VOISubset, Data.Threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gill area (slow way, but with saving it in the middle...)\n",
    "Data['OutputNameAreaGills'] = [os.path.join(f,\n",
    "                                            '%s_area_gills_from_%04d_of_%04d_slices.npy' % (os.path.splitext(os.path.basename(thresholdmethod))[0],\n",
    "                                                                                            len(n[::p]),\n",
    "                                                                                            len(n)))\n",
    "                               for f, n, p, thresholdmethod in zip(Data.Folder,\n",
    "                                                                   Data.VOINames,\n",
    "                                                                   Data.PrintEverySlice,\n",
    "                                                                   Data.OutputNameThresholded)]\n",
    "# Don't save results the dataframe, or else we won't be able to make it :)\n",
    "Data['AreaGills'] = [numpy.nan for file in Data.OutputNameAreaGills]\n",
    "for c, subset in enumerate(Thresholded):\n",
    "    # Only do this if we didn't do it already...\n",
    "    if os.path.exists(Data.OutputNameAreaGills[c]):\n",
    "        print('%2s/%s: %16s: Already saved to %s' % (c + 1,\n",
    "                                                     len(Data.Sample),\n",
    "                                                     Data.Sample[c],\n",
    "                                                     Data.OutputNameAreaGills[c][len(RootFolder):]))\n",
    "    else:\n",
    "        print('%2s/%s: %16s: Calculating gill area on %s of %s VOI images' % (c + 1,\n",
    "                                                                              len(Data.Sample),\n",
    "                                                                              Data.Sample[c],\n",
    "                                                                              len(Data.SubsetNames[c]),\n",
    "                                                                              len(Data.VOINames[c])))\n",
    "        Data.AreaGills[c] = [None] * len(subset)\n",
    "        for d, image in tqdm_notebook(enumerate(subset),\n",
    "                                      total=len(subset),\n",
    "                                      leave=False):\n",
    "            Data.AreaGills[c][d] = calculate_area_gills(image)\n",
    "        print('%23s: Saving area to %s' % (Data.Sample[c], Data.OutputNameAreaGills[c][len(RootFolder):]))\n",
    "        numpy.save(Data.OutputNameAreaGills[c], Data.AreaGills[c], allow_pickle=False)\n",
    "        Data.AreaGills[c] = numpy.nan\n",
    "        # Free up memory\n",
    "        subset._mmap.close()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files again, since we closed them above.\n",
    "Thresholded = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameThresholded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area_organ(image, threshold=None, verbose=False):\n",
    "    \"\"\"\n",
    "    This function tries to estimate an organ area, e.g. the total organ area,\n",
    "    by discarding (e.g filling/closing) the small voids between the secondary filament.\n",
    "    \"\"\"\n",
    "    if numpy.min(image) == False:\n",
    "        thresholded_image = image\n",
    "        if verbose:\n",
    "            print('Input image is already thresholded')\n",
    "    else:\n",
    "        if not threshold:\n",
    "            # Calculate the Otsu threshold of the image if needed\n",
    "            try:\n",
    "                threshold = skimage.filters.threshold_otsu(image)\n",
    "                if verbose:\n",
    "                    print('Thresholding imput image with %0.2f' % threshold)\n",
    "                thresholded_image = image > threshold\n",
    "            except (ValueError):\n",
    "                thresholded_image = numpy.zeros_like(image)\n",
    "    # Use simple binary closing with a large kernel to fill out all the voids\n",
    "    closed = skimage.morphology.binary_closing(thresholded_image,\n",
    "                                               selem=skimage.morphology.selem.disk(11))\n",
    "    if verbose:\n",
    "        # mask out Deas ROI\n",
    "        masked = numpy.ma.masked_equal(image, 0)\n",
    "        # Show the images\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(masked.filled(0))\n",
    "        plt.axis('off')\n",
    "        plt.title('Original')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(thresholded_image)\n",
    "        plt.axis('off')\n",
    "        if numpy.min(image) is False:\n",
    "            plt.title('Input image *is* already thresholded')\n",
    "        else:\n",
    "            plt.title('Thresholded with %0.2f: %0.2g px' % (threshold,\n",
    "                                                            numpy.sum(thresholded_image)))\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(closed)\n",
    "        plt.axis('off')\n",
    "        plt.title('Closed: %0.2g px' % numpy.sum(closed))\n",
    "        plt.show()\n",
    "    area_organ = numpy.sum(closed)\n",
    "    return(area_organ, closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the (extrapolated) organ area (pythonic way, but without saving it in the middle...)\n",
    "# Data['AreaOrgan'] = [[calculate_area_organ(i, threshold=t)\n",
    "#                       for i, t in zip(subset, thrs)]\n",
    "#                      for subset, thrs in zip(VOISubset, Data.Threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (extrapolated) organ area (slow way, but with saving it in the middle...)\n",
    "Data['OutputNameAreaOrgan'] = [os.path.join(f,\n",
    "                                            '%s_area_organ_from_%04d_of_%04d_slices.npy' % (os.path.splitext(os.path.basename(thresholdmethod))[0],\n",
    "                                                                                            len(n[::p]),\n",
    "                                                                                            len(n)))\n",
    "                               for f, n, p, thresholdmethod in zip(Data.Folder,\n",
    "                                                                   Data.VOINames,\n",
    "                                                                   Data.PrintEverySlice,\n",
    "                                                                   Data.OutputNameThresholded)]\n",
    "Data['OutputNameOrgan'] = [os.path.join(f,\n",
    "                                        '%s_organ_from_%04d_of_%04d_slices.npy' % (os.path.splitext(os.path.basename(thresholdmethod))[0],\n",
    "                                                                                   len(n[::p]),\n",
    "                                                                                   len(n)))\n",
    "                           for f, n, p, thresholdmethod in zip(Data.Folder,\n",
    "                                                               Data.VOINames,\n",
    "                                                               Data.PrintEverySlice,\n",
    "                                                               Data.OutputNameThresholded)]\n",
    "# Don't save the resulting images into the dataframe, or else we won't be able to make it :)\n",
    "Data['AreaOrgan'] = [numpy.nan for file in Data.OutputNameAreaOrgan]\n",
    "Closed = [numpy.nan for file in Data.OutputNameAreaOrgan]\n",
    "for c, subset in enumerate(Thresholded):\n",
    "    # Only do this if we didn't do it already...\n",
    "    if os.path.exists(Data.OutputNameAreaOrgan[c]) and os.path.exists(Data.OutputNameOrgan[c]):\n",
    "        print('%2s/%s: %16s: Already saved to %s' % (c + 1,\n",
    "                                                     len(Data.Sample),\n",
    "                                                     Data.Sample[c],\n",
    "                                                     Data.OutputNameAreaOrgan[c][len(RootFolder):]))\n",
    "    else:\n",
    "        print('%2s/%s: %16s: Calculating organ area on %s of %s VOI images' % (c + 1,\n",
    "                                                                               len(Data.Sample),\n",
    "                                                                               Data.Sample[c],\n",
    "                                                                               len(Data.SubsetNames[c]),\n",
    "                                                                               len(Data.VOINames[c])))\n",
    "        Data.AreaOrgan[c] = [None] * len(subset)\n",
    "        Closed[c] = [None] * len(subset)\n",
    "        for d, image in tqdm_notebook(enumerate(subset),\n",
    "                                      total=len(subset),\n",
    "                                      leave=False):\n",
    "            Data['AreaOrgan'][c][d], Closed[c][d] = calculate_area_organ(image)\n",
    "        print('%s: Saving area to %s' % (os.path.basename(sample.Sample.rjust(namelenmax + 7)),\n",
    "                                         Data.OutputNameAreaOrgan[c][len(RootFolder):]))\n",
    "        numpy.save(Data.OutputNameAreaOrgan[c],\n",
    "                   Data['AreaOrgan'][c],\n",
    "                   allow_pickle=False)\n",
    "        print('%s: Saving closed images to %s' % (Data.Sample[c].rjust(namelenmax + 7),\n",
    "                                                  Data.OutputNameOrgan[c][len(RootFolder):]))\n",
    "        numpy.save(Data['OutputNameOrgan'][c],\n",
    "                   Closed[c],\n",
    "                   allow_pickle=False)\n",
    "        Data.AreaOrgan[c] = numpy.nan\n",
    "        Closed[c] = numpy.nan\n",
    "        # Free up memory\n",
    "        subset._mmap.close()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files again, since we closed them above.\n",
    "if Data.PrintEverySlice[0] == 1:\n",
    "    # If we did NOT use a subset, load the original stack...\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOI]\n",
    "else:\n",
    "    # If we did use a subset, then load the subset\n",
    "    VOISubset = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameVOISubset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (or memory-map) all the files again, since we closed them above.\n",
    "Closed = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameOrgan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save organ area images out\n",
    "for c, sample in Data.iterrows():\n",
    "    sample['CurrentOutputFolder'] = os.path.splitext(Data.OutputNameThresholded[c])[0].replace(Data.Sample[c] + '_gills', 'VOI_organ_area')\n",
    "    if not os.path.exists(sample.CurrentOutputFolder):\n",
    "        os.makedirs(sample.CurrentOutputFolder)\n",
    "    if len(glob.glob(os.path.join(sample.CurrentOutputFolder, '*.png'))) >= sample.NumberOfAnalyzedVOISlices:\n",
    "        print('%2s/%s: %7s: Already saved %3s organ area images to %s' % (c + 1,\n",
    "                                                                          len(Data),\n",
    "                                                                          sample.Sample,\n",
    "                                                                          sample.NumberOfAnalyzedVOISlices,\n",
    "                                                                          sample.CurrentOutputFolder[len(RootFolder):]))\n",
    "    else:\n",
    "        print('%2s/%s: %7s: Saving %3s organ area images to %s' % (c + 1,\n",
    "                                                                   len(Data),\n",
    "                                                                   Data.Sample[c],\n",
    "                                                                   sample.NumberOfAnalyzedVOISlices,\n",
    "                                                                   sample['CurrentOutputFolder'][len(RootFolder):]))\n",
    "        for d, i in tqdm_notebook(enumerate(Closed[c]),\n",
    "                                  total=len(Closed[c]),\n",
    "                                  leave=False):\n",
    "            scipy.misc.imsave(os.path.join(sample.CurrentOutputFolder,\n",
    "                                           sample.Sample + '_area_organ_%04d.png' % d),\n",
    "                              i.astype('uint8') * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in\n",
    "Data['AreaGills'] = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameAreaGills]\n",
    "Data['AreaOrgan'] = [numpy.load(file, mmap_mode='r') for file in Data.OutputNameAreaOrgan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     print(80*'-')\n",
    "#     print(row.Sample)\n",
    "#     print(row.AreaGills)\n",
    "#     print(row.AreaOrgan)\n",
    "#     print(row.AreaGills / row.AreaOrgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the images to get the volume of the gills and organ\n",
    "# This volume is obviously in voxels\n",
    "Data['VolumeGills'] = [numpy.sum(ag) for ag in Data.AreaGills]\n",
    "Data['VolumeOrgan'] = [numpy.sum(ao) for ao in Data.AreaOrgan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the volume in microliters (or mm^3)\n",
    "Data['VolumeGills_ul'] = [vg * vv for vg, vv in zip(Data.VolumeGills, Data.VoxelVolume)]\n",
    "Data['VolumeOrgan_ul'] = [vo * vv for vo, vv in zip(Data.VolumeOrgan, Data.VoxelVolume)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gills per organ ratio\n",
    "Data['GillsPerOrgan'] = [ag / ao for (ag, ao) in zip(Data['AreaGills'], Data['AreaOrgan'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display data\n",
    "# for c, row in Data.iterrows():\n",
    "#     plt.subplot(121)\n",
    "#     plt.plot(row.AreaGills)\n",
    "#     plt.plot(row.AreaOrgan)\n",
    "#     plt.subplot(122)\n",
    "#     plt.plot(row.GillsPerOrgan)\n",
    "#     plt.title(row.Sample)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Areas\n",
    "for c, a in enumerate(Data.GillsPerOrgan):\n",
    "    plt.subplot(1, len(Data), c + 1)\n",
    "    seaborn.violinplot(a, orient='v', color=Data.Color[c], cut=0)\n",
    "    if c:\n",
    "        plt.gca().axes.yaxis.set_ticklabels([])\n",
    "    else:\n",
    "        plt.ylabel('Gills per organ ratio')\n",
    "    plt.xlabel(Data.Sample[c], rotation=90)\n",
    "plt.suptitle('\"Gills per organ\" ratio for all images')\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Gills_per_organ_from%04dslices.png' % NumberOfImagesToShow),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean 'gills per organ' ratio\n",
    "Data['Mean_Gill_Ratio'] = [float(numpy.nanmean(a)) for a in Data['GillsPerOrgan']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gill volume normalized to the fish length\n",
    "Data['VolumeGillsNormalized'] = [numpy.divide(gv, w) for gv, w in zip(Data.VolumeGills_ul, Data.Length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data let's display all that we need!\n",
    "The plots below are the ones that are shown in the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change color palette from the default\n",
    "# seaborn.set_palette(seaborn.color_palette('muted', 2))\n",
    "# seaborn.set_palette(seaborn.color_palette('viridis', 2))\n",
    "# seaborn.set_palette(seaborn.color_palette('hls', 2))\n",
    "# seaborn.set_palette(seaborn.color_palette('Spectral', 2))\n",
    "# seaborn.set_palette(seaborn.color_palette('PiYG', 2))\n",
    "# seaborn.set_palette(seaborn.color_palette('cubehelix', 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Swimming activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_training = pandas.concat([pandas.DataFrame(index=Speed.Training.unique(),\n",
    "                                                    columns=Speed.Experiment.unique()),\n",
    "                                   pandas.DataFrame(index=itertools.combinations(Speed.Training.unique(), 2),\n",
    "                                                    columns=Speed.Experiment.unique())])\n",
    "p_values_training['Control vs. Swimmer'] = numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there differences between control and swimmers?\n",
    "for training in Speed.Training.unique():\n",
    "    print(80 * '-')\n",
    "    t_statistic, p_value = scipy.stats.ttest_ind(Speed[(Speed.Training == training)\n",
    "                                                       & (Speed.Experiment == 'Control')].Seconds.dropna(),\n",
    "                                                 Speed[(Speed.Training == training)\n",
    "                                                       & (Speed.Experiment == 'Swimmer')].Seconds.dropna(),\n",
    "                                                 equal_var=True)\n",
    "    # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "    p_values_training['Control vs. Swimmer'][training] = p_value / 2\n",
    "    print(\"At %s, the duration for 'Control' and 'Swimmer' has an\" % training)\n",
    "    print('F value of %0.4s and a p value of %.2g (%s).' % (t_statistic,\n",
    "                                                            p_values_training['Control vs. Swimmer'][training],\n",
    "                                                            significance(p_values_training['Control vs. Swimmer'][training])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the durations between trainings different?\n",
    "for experiment in Speed.Experiment.unique():\n",
    "    for trainings in itertools.combinations(Speed.Training.unique(), 2):\n",
    "        print(80 * '-')\n",
    "        t_statistic, p_value = scipy.stats.ttest_ind(Speed[(Speed.Training == trainings[0])\n",
    "                                                           & (Speed.Experiment == experiment)].Seconds.dropna(),\n",
    "                                                     Speed[(Speed.Training == trainings[1])\n",
    "                                                           & (Speed.Experiment == experiment)].Seconds.dropna(),\n",
    "                                                     equal_var=True)\n",
    "        # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "        p_values_training[experiment][trainings] = p_value / 2\n",
    "        print(\"For the %s group, the data between '%s' and '%s' has an\" % (experiment,\n",
    "                                                                           trainings[0],\n",
    "                                                                           trainings[1]))\n",
    "        print('F value of %0.4s and a p value of %.2g. (%s)' % (t_statistic,\n",
    "                                                                p_values_training[experiment][trainings],\n",
    "                                                                significance(p_values_training[experiment][trainings])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the speed data\n",
    "import matplotlib.dates as mdates\n",
    "seaborn.violinplot(data=Speed, hue='Experiment', y='Seconds', x='Training', cut=0, inner='quartiles')\n",
    "seaborn.stripplot(data=Speed, hue='Experiment', y='Seconds', x='Training',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  palette=['gray', 'gray'])\n",
    "legend = True\n",
    "if legend:\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles[:len(handles) // 2],\n",
    "               labels[:len(labels) // 2],\n",
    "               loc='lower right')\n",
    "else:\n",
    "    plt.legend().set_visible(False)\n",
    "plt.ylabel('Swimming performance [s]')\n",
    "significance_bar(0 + 0.2,\n",
    "                 1 + 0.2,\n",
    "                 numpy.maximum(Speed[(Speed.Experiment == 'Swimmer')\n",
    "                                     & (Speed.Training == 'Before')].Seconds.max(),\n",
    "                               Speed[(Speed.Experiment == 'Swimmer')\n",
    "                                     & (Speed.Training == '3 wk')].Seconds.max()),\n",
    "                 p_values_training['Swimmer'][-3])\n",
    "significance_bar(2 - 0.2,\n",
    "                 2 + 0.2,\n",
    "                 Speed[(Speed.Training == '5 wk')].Seconds.max(),\n",
    "                 p_values_training['Control vs. Swimmer']['5 wk'])\n",
    "significance_bar(0 + 0.2,\n",
    "                 2 + 0.2,\n",
    "                 numpy.maximum(Speed[(Speed.Experiment == 'Swimmer')\n",
    "                                     & (Speed.Training == 'Before')].Seconds.max(),\n",
    "                               Speed[(Speed.Experiment == 'Swimmer')\n",
    "                                     & (Speed.Training == '5 wk')].Seconds.max()),\n",
    "                 p_values_training['Swimmer'][-2])\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Endurance.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "for experiment in Speed.Experiment.unique():\n",
    "    print(80 * '-')\n",
    "    for training in Speed.Training.unique():\n",
    "        print('The endurance of the %s group at %s training was %s min %s sec '\n",
    "              'with a STD of %s min %s sec.' % (experiment,\n",
    "                                                training,\n",
    "                                                pandas.to_datetime(Speed[(Speed.Experiment == experiment)\n",
    "                                                                         & (Speed.Training == training)].Seconds.mean(),\n",
    "                                                                   unit='s').minute,\n",
    "                                                pandas.to_datetime(Speed[(Speed.Experiment == experiment)\n",
    "                                                                         & (Speed.Training == training)].Seconds.mean(),\n",
    "                                                                   unit='s').second,\n",
    "                                                pandas.to_datetime(Speed[(Speed.Experiment == experiment)\n",
    "                                                                         & (Speed.Training == training)].Seconds.std(),\n",
    "                                                                   unit='s').minute,\n",
    "                                                pandas.to_datetime(Speed[(Speed.Experiment == experiment)\n",
    "                                                                         & (Speed.Training == training)].Seconds.std(),\n",
    "                                                                   unit='s').second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Speed.Experiment.unique():\n",
    "    print(80 * '-')\n",
    "    for trainings in itertools.combinations(Speed.Training.unique(), 2):\n",
    "        print('The speed of the %s '\n",
    "              'increased by %0.2f %% between '\n",
    "              '\"%s\" and \"%s\"' % (experiment,\n",
    "                                 percentage(Speed[(Speed.Experiment == experiment)\n",
    "                                                  & (Speed.Training == trainings[0])].Seconds.mean(),\n",
    "                                            Speed[(Speed.Experiment == experiment)\n",
    "                                                  & (Speed.Training == trainings[1])].Seconds.mean()),\n",
    "                                 trainings[0],\n",
    "                                 trainings[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_length = [None] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there length differences between control and swimmers?\n",
    "for c, training in enumerate(Morphology.Training.unique()):\n",
    "    print(80 * '-')\n",
    "    t_statistic, p_values_length[c] = scipy.stats.ttest_ind(Morphology[(Morphology.Training == training)\n",
    "                                                                       & (Morphology.Experiment == 'Control')].Length.dropna(),\n",
    "                                                            Morphology[(Morphology.Training == training)\n",
    "                                                                       & (Morphology.Experiment == 'Swimmer')].Length.dropna(),\n",
    "                                                            equal_var=True)\n",
    "    print(\"At %s, the 'lengths' for 'Control' and 'Swimmer' have a\" % training)\n",
    "    # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "    p_values_length[c] /= 2\n",
    "    print('F value of %0.4s and a p value of %.2g (%s).' % (t_statistic,\n",
    "                                                            p_values_length[c],\n",
    "                                                            significance(p_values_length[c])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the lengths between trainings different?\n",
    "for c, experiment in enumerate(Morphology.Experiment.unique()):\n",
    "    for trainings in itertools.combinations(Morphology.Training.unique(), 2):\n",
    "        print(80 * '-')\n",
    "        t_statistic, p_values_length[c + 2] = scipy.stats.ttest_ind(Morphology[(Morphology.Training == trainings[0])\n",
    "                                                                               & (Morphology.Experiment == experiment)].Length.dropna(),\n",
    "                                                                    Morphology[(Morphology.Training == trainings[1])\n",
    "                                                                               & (Morphology.Experiment == experiment)].Length.dropna(),\n",
    "                                                                    equal_var=True)\n",
    "        print(\"For the %s group, the 'length' between '%s' and '%s' has an\" % (experiment,\n",
    "                                                                               trainings[0],\n",
    "                                                                               trainings[1]))\n",
    "        # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "        p_values_length[c + 2] /= 2\n",
    "        print('F value of %0.4s and a p value of %.2g. (%s)' % (t_statistic,\n",
    "                                                                p_values_length[c + 2],\n",
    "                                                                significance(p_values_length[c + 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display fish length\n",
    "seaborn.violinplot(data=Morphology, hue='Experiment', y='Length', x='Training', cut=0, inner='quartiles')\n",
    "seaborn.stripplot(data=Morphology, hue='Experiment', y='Length', x='Training',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  palette=['gray', 'gray'])\n",
    "if legend:\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles[:len(handles) // 2],\n",
    "               labels[:len(labels) // 2],\n",
    "               loc='upper left')\n",
    "else:\n",
    "    plt.legend().set_visible(False)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Fish length [mm]')\n",
    "\n",
    "plt.gca().set_xticklabels(['Before Training', 'After Training'])\n",
    "significance_bar(0 + 0.2,\n",
    "                 1 + 0.2,\n",
    "                 Morphology[(Morphology.Experiment == 'Swimmer')].Length.max(),\n",
    "                 p_values_length[-1])\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Lengths.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "for experiment in Morphology.Experiment.unique():\n",
    "    print(80 * '-')\n",
    "    for training in Morphology.Training.unique():\n",
    "        print('The length of the %s group %s training '\n",
    "              'was %0.2f mm with a STD of %0.2f.' % (experiment,\n",
    "                                                     training,\n",
    "                                                     Morphology[(Morphology.Experiment == experiment)\n",
    "                                                                & (Morphology.Training == training)].Length.mean(),\n",
    "                                                     Morphology[(Morphology.Experiment == experiment)\n",
    "                                                                & (Morphology.Training == training)].Length.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Morphology.Experiment.unique():\n",
    "    for trainings in itertools.combinations(Morphology.Training.unique(), 2):\n",
    "        print('The length of the %s '\n",
    "              'increased by %0.2f %% between '\n",
    "              '\"%s\" and \"%s\"' % (experiment,\n",
    "                                 percentage(Morphology[(Morphology.Experiment == experiment)\n",
    "                                                       & (Morphology.Training == trainings[0])].Length.mean(),\n",
    "                                            Morphology[(Morphology.Experiment == experiment)\n",
    "                                                       & (Morphology.Training == trainings[1])].Length.mean()),\n",
    "                                 trainings[0],\n",
    "                                 trainings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_weight = [None] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there weight differences between control and swimmers?\n",
    "for c, training in enumerate(Morphology.Training.unique()):\n",
    "    print(80 * '-')\n",
    "    t_statistic, p_values_weight[c] = scipy.stats.ttest_ind(Morphology[(Morphology.Gender == 'Male')\n",
    "                                                                       & (Morphology.Training == training)\n",
    "                                                                       & (Morphology.Experiment == 'Control')].Weight.dropna(),\n",
    "                                                            Morphology[(Morphology.Gender == 'Male')\n",
    "                                                                       & (Morphology.Training == training)\n",
    "                                                                       & (Morphology.Experiment == 'Swimmer')].Weight.dropna(),\n",
    "                                                            equal_var=True)\n",
    "    print(\"At %s, the weight for 'Control' and 'Swimmer' has an\" % training)\n",
    "    # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "    p_values_weight[c] /= 2\n",
    "    print('F value of %0.4s and a p value of %.2g (%s).' % (t_statistic,\n",
    "                                                            p_values_weight[c],\n",
    "                                                            significance(p_values_weight[c])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the weights between trainings different?\n",
    "for c, experiment in enumerate(Morphology.Experiment.unique()):\n",
    "    for trainings in itertools.combinations(Morphology.Training.unique(), 2):\n",
    "        print(80 * '-')\n",
    "        t_statistic, p_values_weight[c + 2] = scipy.stats.ttest_ind(Morphology[(Morphology.Gender == 'Male')\n",
    "                                                                               & (Morphology.Training == trainings[0])\n",
    "                                                                               & (Morphology.Experiment == experiment)].Length.dropna(),\n",
    "                                                                    Morphology[(Morphology.Gender == 'Male')\n",
    "                                                                               & (Morphology.Training == trainings[1])\n",
    "                                                                               & (Morphology.Experiment == experiment)].Length.dropna(),\n",
    "                                                                    equal_var=True)\n",
    "        print(\"For the %s group, the weight between '%s' and '%s' has an\" % (experiment,\n",
    "                                                                             trainings[0],\n",
    "                                                                             trainings[1]))\n",
    "        # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "        p_values_weight[c + 2] /= 2\n",
    "        print('F value of %0.4s and a p value of %.2g. (%s)' % (t_statistic,\n",
    "                                                                p_values_weight[c + 2],\n",
    "                                                                significance(p_values_weight[c + 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display male fish weight\n",
    "seaborn.violinplot(data=Morphology[Morphology.Gender == 'Male'], hue='Experiment', y='Weight', x='Training',\n",
    "                   cut=0, inner='quartiles')\n",
    "seaborn.stripplot(data=Morphology[Morphology.Gender == 'Male'], hue='Experiment', y='Weight', x='Training',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  palette=['gray', 'gray'])\n",
    "if legend:\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles[:len(handles) // 2],\n",
    "               labels[:len(labels) // 2],\n",
    "               loc='upper left')\n",
    "else:\n",
    "    plt.legend().set_visible(False)\n",
    "plt.xlabel('')\n",
    "plt.gca().set_xticklabels(['Before Training', 'After Training'])\n",
    "plt.ylabel('Fish weight [g]')\n",
    "significance_bar(1 - 0.2,\n",
    "                 1 + 0.2,\n",
    "                 Morphology[(Morphology.Gender == 'Male')\n",
    "                            & (Morphology.Training == 'After')].Weight.max(),\n",
    "                 p_values_weight[1])\n",
    "significance_bar(0 + 0.2,\n",
    "                 1 + 0.2,\n",
    "                 Morphology[(Morphology.Gender == 'Male')\n",
    "                            & (Morphology.Experiment == 'Swimmer')].Weight.max(),\n",
    "                 p_values_weight[-1])\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Weights.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "for experiment in Morphology.Experiment.unique():\n",
    "    print(80 * '-')\n",
    "    for training in Morphology.Training.unique():\n",
    "        print('The weight of the male %s group %s training '\n",
    "              'was %0.3f g with a STD of %0.3f.' % (experiment,\n",
    "                                                    training,\n",
    "                                                    Morphology[(Morphology.Gender == 'Male')\n",
    "                                                               & (Morphology.Experiment == experiment)\n",
    "                                                               & (Morphology.Training == training)].Weight.mean(),\n",
    "                                                    Morphology[(Morphology.Gender == 'Male')\n",
    "                                                               & (Morphology.Experiment == experiment)\n",
    "                                                               & (Morphology.Training == training)].Weight.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Morphology.Experiment.unique():\n",
    "    for trainings in itertools.combinations(Morphology.Training.unique(), 2):\n",
    "        print('The weight of the male %s '\n",
    "              'increased by %0.2f %% between '\n",
    "              '\"%s\" and \"%s\"' % (experiment,\n",
    "                                 percentage(Morphology[(Morphology.Gender == 'Male')\n",
    "                                                       & (Morphology.Experiment == experiment)\n",
    "                                                       & (Morphology.Training == trainings[0])].Weight.mean(),\n",
    "                                            Morphology[(Morphology.Gender == 'Male')\n",
    "                                                       & (Morphology.Experiment == experiment)\n",
    "                                                       & (Morphology.Training == trainings[1])].Weight.mean()),\n",
    "                                 trainings[0],\n",
    "                                 trainings[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Scanning electron microscopy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the filament counts between trainings different?\n",
    "\n",
    "t_statistic, p_value_filament_count = scipy.stats.ttest_ind(Filaments['Control Count'].dropna(),\n",
    "                                                            Filaments['Swimmer Count'].dropna(),\n",
    "                                                            equal_var=True)\n",
    "print(\"The filament count between 'Control' and 'Swimmer' has an\")\n",
    "# Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "p_value_filament_count /= 2\n",
    "print('F value of %0.4s and a p value of %.2g. (%s)' % (t_statistic,\n",
    "                                                        p_value_filament_count,\n",
    "                                                        significance(p_value_filament_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display filament count\n",
    "seaborn.violinplot(data=Filaments[['Control Count', 'Swimmer Count']],\n",
    "                   cut=0, inner='quartiles')\n",
    "# seaborn.boxplot(data=Filaments[['Control Count', 'Swimmer Count']], notch=True)\n",
    "seaborn.stripplot(data=Filaments[['Control Count', 'Swimmer Count']],\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  color='gray')\n",
    "significance_bar(0,\n",
    "                 1,\n",
    "                 numpy.max(Filaments[['Control Count', 'Swimmer Count']].max()),\n",
    "                 p_value_filament_count)\n",
    "plt.ylabel('Secondary filament count\\nof the five primary filaments')\n",
    "plt.gca().set_xticklabels(['Control', 'Swimmer'])\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Filament_count.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "for experiment in Morphology.Experiment.unique():\n",
    "    print(80 * '-')\n",
    "    for training in Morphology.Training.unique():\n",
    "        print('The length of the %s group %s training '\n",
    "              'was %0.2f mm with a STD of %0.2f.' % (experiment,\n",
    "                                                     training,\n",
    "                                                     Morphology[(Morphology.Experiment == experiment)\n",
    "                                                                & (Morphology.Training == training)].Length.mean(),\n",
    "                                                     Morphology[(Morphology.Experiment == experiment)\n",
    "                                                                & (Morphology.Training == training)].Length.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "print('The mean secondary filament count for the '\n",
    "      'control group was %0.2f with a STD of %0.2f.' % (Filaments['Control Count'].mean(),\n",
    "                                                        Filaments['Control Count'].std()))\n",
    "print('The mean secondary filament count for the '\n",
    "      'swimmers group was %0.2f with a STD of %0.2f.' % (Filaments['Swimmer Count'].mean(),\n",
    "                                                         Filaments['Swimmer Count'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is an increase of %0.2f %%' % percentage(Filaments['Control Count'].mean(), Filaments['Swimmer Count'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the filament lenghts between trainings different?\n",
    "t_statistic, p_value_filament_length = scipy.stats.ttest_ind(Filaments['Control Length'].dropna(),\n",
    "                                                             Filaments['Swimmer Length'].dropna(),\n",
    "                                                             equal_var=True)\n",
    "print(\"The filament length between 'Control' and 'Swimmer' has an\")\n",
    "# Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "p_value_filament_length /= 2\n",
    "print('F value of %0.4s and a p value of %.2g. (%s)' % (t_statistic,\n",
    "                                                        p_value_filament_length,\n",
    "                                                        significance(p_value_filament_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display filament length\n",
    "seaborn.violinplot(data=Filaments[['Control Length', 'Swimmer Length']],\n",
    "                   cut=0, inner='quartiles')\n",
    "seaborn.stripplot(data=Filaments[['Control Length', 'Swimmer Length']],\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  color='gray')\n",
    "significance_bar(0, 1,\n",
    "                 numpy.max(Filaments[['Control Length', 'Swimmer Length']].max()),\n",
    "                 p_value_filament_length)\n",
    "plt.ylabel(u'Primary filament length [\\u03bcm]')\n",
    "plt.gca().set_xticklabels(['Control', 'Swimmer'])\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Filament_lenght.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "print('The mean filament length for the control was %0.f um with a STD of %0.f.' % (Filaments['Control Length'].mean(),\n",
    "                                                                                    Filaments['Control Length'].std()))\n",
    "print('The mean filament length for the swimmers was %0.f um with a STD of %0.f.' % (Filaments['Swimmer Length'].mean(),\n",
    "                                                                                     Filaments['Swimmer Length'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is an increase of %0.2f %%' % percentage(Filaments['Control Length'].mean(), Filaments['Swimmer Length'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Gill volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro -> Normalittstest\n",
    "scipy.stats.shapiro(Data.VolumeGills)\n",
    "# nicht signifikant von Normalverteilung unterschiedlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levene -> Varianztest\n",
    "scipy.stats.levene(Data.loc[Data.Experiment == 'Control'].VolumeGills,\n",
    "                   Data.loc[Data.Experiment == 'Swimmer'].VolumeGills)\n",
    "# nicht signifikant -> Wahrscheinlich nicht nicht normalverteilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Kolmogorov-Smirnov statistic on 2 samples -> Verteilungen nicht unterschiedlich?\n",
    "scipy.stats.ks_2samp(Data.loc[Data.Experiment == 'Control'].VolumeGills,\n",
    "                     Data.loc[Data.Experiment == 'Swimmer'].VolumeGills)\n",
    "# nicht signifikant -> wahrscheinlich nicht unterschiedliche Verteilung --> equal_var=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the means different?\n",
    "t_statistic, p_value_volume = scipy.stats.ttest_ind(Data.loc[Data.Experiment == 'Control'].VolumeGills,\n",
    "                                                    Data.loc[Data.Experiment == 'Swimmer'].VolumeGills,\n",
    "                                                    equal_var=True)\n",
    "print(\"The difference between the 'gill volume' of 'Control' and 'Swimmer' has an\")\n",
    "# Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "p_value_volume /= 2\n",
    "print('F value of %0.4s and a p value of %.2g (%s).' % (t_statistic,\n",
    "                                                        p_value_volume,\n",
    "                                                        significance(p_value_volume)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the gills volume for each animal (in ul)\n",
    "bars = seaborn.violinplot(data=Data, x='Experiment', y='VolumeGills_ul', cut=0, inner='quartiles')\n",
    "# bars = seaborn.boxplot(data=Data, x='Experiment', y='VolumeGills_ul')\n",
    "seaborn.stripplot(data=Data, x='Experiment', y='VolumeGills_ul',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  color='gray')\n",
    "label = False\n",
    "if label:\n",
    "    shift = 0.05\n",
    "    for c, row in Data.iterrows():\n",
    "        if Data.Experiment[c] == 'Control':\n",
    "            plt.gca().annotate(Data.Sample[c],\n",
    "                               (0 + shift * numpy.random.rand(),\n",
    "                                Data.VolumeGills_ul[c]),\n",
    "                               horizontalalignment='left', verticalalignment='bottom')\n",
    "        else:\n",
    "            plt.gca().annotate(Data.Sample[c],\n",
    "                               (1 + shift * numpy.random.rand(),\n",
    "                                Data.VolumeGills_ul[c]),\n",
    "                               horizontalalignment='left', verticalalignment='bottom')\n",
    "plt.xlabel('')\n",
    "plt.ylabel(u'Gill volume [\\u03bcl]')  # https://stackoverflow.com/a/2140991/323100\n",
    "plt.ylim(ymax=Data.VolumeGills_ul.max() * 1.1)\n",
    "significance_bar(0, 1,\n",
    "                 Data.VolumeGills_ul.max(),\n",
    "                 p_value_volume)\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "if label:\n",
    "    plt.savefig(os.path.join(OutPutDir,\n",
    "                             'Volume_Gills_ul_from%04dslices.png' % NumberOfImagesToShow),\n",
    "                bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(os.path.join(OutPutDir,\n",
    "                             'Volume_Gills_ul_from%04dslices_nolabel.png' % NumberOfImagesToShow),\n",
    "                bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data as prose\n",
    "for experiment in Data.Experiment.unique():\n",
    "    print('The %s fishes have a mean gill volume '\n",
    "          'of %0.3f mm^3' % (experiment,\n",
    "                             Data[Data.Experiment == experiment]['VolumeGills_ul'].mean()))\n",
    "print('This is an increase of %0.2f %%' % percentage(Data['VolumeGills_ul'][Data.Experiment == 'Control'].mean(),\n",
    "                                                     Data['VolumeGills_ul'][Data.Experiment == 'Swimmer'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Gill volume, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro -> Normalittstest\n",
    "scipy.stats.shapiro(Data.VolumeGillsNormalized)\n",
    "# nicht signifikant von Normalverteilung unterschiedlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levene -> Varianztest\n",
    "scipy.stats.levene(Data.loc[Data.Experiment == 'Control'].VolumeGillsNormalized,\n",
    "                   Data.loc[Data.Experiment == 'Swimmer'].VolumeGillsNormalized)\n",
    "# nicht signifikant -> Wahrscheinlich nicht nicht normalverteilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Kolmogorov-Smirnov statistic on 2 samples -> Verteilungen nicht unterschiedlich?\n",
    "scipy.stats.ks_2samp(Data.loc[Data.Experiment == 'Control'].VolumeGillsNormalized,\n",
    "                     Data.loc[Data.Experiment == 'Swimmer'].VolumeGillsNormalized)\n",
    "# knapp nicht signifikant -> wahrscheinlich nicht unterschiedliche Verteilung --> equal_var=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the means different?\n",
    "t_statistic, p_value_volume_normalized = scipy.stats.ttest_ind(Data.loc[Data.Experiment == 'Control'].VolumeGillsNormalized,\n",
    "                                                               Data.loc[Data.Experiment == 'Swimmer'].VolumeGillsNormalized,\n",
    "                                                               equal_var=True)\n",
    "print(\"The difference between the 'normalized gill volume' of 'Control' and 'Swimmer' has an\")\n",
    "# Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "p_value_volume_normalized /= 2\n",
    "print('F value of %0.4s and a p value of %.2g.' % (t_statistic,\n",
    "                                                   p_value_volume_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of the NORMALIZED gill volumes\n",
    "seaborn.violinplot(data=Data, x='Experiment', y='VolumeGillsNormalized', cut=0, inner='quartiles')\n",
    "# seaborn.boxplot(data=Data, x='Experiment', y='VolumeGillsNormalized')\n",
    "seaborn.stripplot(data=Data, x='Experiment', y='VolumeGillsNormalized',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  color='gray')\n",
    "label = False\n",
    "if label:\n",
    "    shift = 0.05\n",
    "    for c, row in Data.iterrows():\n",
    "        if Data.Experiment[c] == 'Control':\n",
    "            plt.gca().annotate(Data.Sample[c],\n",
    "                               (0 + shift * numpy.random.rand(),\n",
    "                                Data.VolumeGillsNormalized[c]),\n",
    "                               horizontalalignment='left', verticalalignment='bottom')\n",
    "        else:\n",
    "            plt.gca().annotate(Data.Sample[c],\n",
    "                               (1 + shift * numpy.random.rand(),\n",
    "                                Data.VolumeGillsNormalized[c]),\n",
    "                               horizontalalignment='left', verticalalignment='bottom')\n",
    "# plt.title('Normalized gill volume (normalized to fish length)')\n",
    "# plt.ylim(ymax=Data.VolumeGillsNormalized.max() * 1.2)\n",
    "plt.xlabel('')\n",
    "plt.ylabel(u'Normalized gill volume [\\u03bcl/mm]')  # https://stackoverflow.com/a/2140991/323100\n",
    "# significance_bar(0, 1,\n",
    "#                  Data.VolumeGillsNormalized.max(),\n",
    "#                  p_value_volume_normalized)\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "if label:\n",
    "    plt.savefig(os.path.join(OutPutDir,\n",
    "                             'Volume_Gills_Normalized_ul_from%04dslices.png' % NumberOfImagesToShow),\n",
    "                bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(os.path.join(OutPutDir,\n",
    "                             'Volume_Gills_Normalized_ul_from%04slies_nolabels.png' % NumberOfImagesToShow),\n",
    "                bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Gills per organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro -> Normalittstest\n",
    "scipy.stats.shapiro(Data.Mean_Gill_Ratio)\n",
    "# nicht signifikant von Normalverteilung unterschiedlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levene -> Varianztest\n",
    "scipy.stats.levene(Data.loc[Data.Experiment == 'Control'].Mean_Gill_Ratio,\n",
    "                   Data.loc[Data.Experiment == 'Swimmer'].Mean_Gill_Ratio)\n",
    "# nicht signifikant -> Wahrscheinlich nicht nicht normalverteilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Kolmogorov-Smirnov statistic on 2 samples -> Verteilungen nicht unterschiedlich?\n",
    "scipy.stats.ks_2samp(Data.loc[Data.Experiment == 'Control'].Mean_Gill_Ratio,\n",
    "                     Data.loc[Data.Experiment == 'Swimmer'].Mean_Gill_Ratio)\n",
    "# nicht signifikant -> wahrscheinlich nicht unterschiedliche Verteilung --> equal_var=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the means different?\n",
    "t_statistic, p_value_area = scipy.stats.ttest_ind(Data.loc[Data.Experiment == 'Control'].Mean_Gill_Ratio,\n",
    "                                                  Data.loc[Data.Experiment == 'Swimmer'].Mean_Gill_Ratio,\n",
    "                                                  equal_var=True)\n",
    "print(\"The difference between the 'Mean_Gill_Ratio' of 'Control' and 'Swimmer' has an\")\n",
    "# Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "p_value_area /= 2\n",
    "print('F value of %0.4s and a p value of %.2g.' % (t_statistic,\n",
    "                                                   p_value_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Boxplot of the mean area for each animal (calculated slice-wise)\n",
    "seaborn.violinplot(data=Data, x='Experiment', y='Mean_Gill_Ratio', cut=0, inner='quartiles')\n",
    "seaborn.stripplot(data=Data, x='Experiment', y='Mean_Gill_Ratio',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  color='gray')\n",
    "label = False\n",
    "if label:\n",
    "    shift = 0.1\n",
    "    for c, row in Data.iterrows():\n",
    "        if Data.Experiment[c] == 'Control':\n",
    "            plt.gca().annotate(Data.Sample[c], (0 + shift * numpy.random.rand(),\n",
    "                                                Data.Mean_Gill_Ratio[c]),\n",
    "                               horizontalalignment='left', verticalalignment='bottom')\n",
    "        else:\n",
    "            plt.gca().annotate(Data.Sample[c], (1 + shift * numpy.random.rand(),\n",
    "                                                Data.Mean_Gill_Ratio[c]),\n",
    "                               horizontalalignment='left', verticalalignment='bottom')\n",
    "# plt.title('Average \"gills per organ\" from slices')\n",
    "plt.ylim(ymax=Data.Mean_Gill_Ratio.max() * 1.1)\n",
    "plt.ylabel('Gills per organ')\n",
    "significance_bar(0, 1,\n",
    "                 Data.Mean_Gill_Ratio.max(),\n",
    "                 p_value_area)\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir,\n",
    "                         'Gills_per_organ_average_slices_from%04dslices.png' % NumberOfImagesToShow),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Respirometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_respirometry = [None] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there differences between control and swimmers?\n",
    "for c, training in enumerate(Morphology.Training.unique()):\n",
    "    print(80 * '-')\n",
    "    t_statistic, p_value_respirometry[c] = scipy.stats.ttest_ind(Respirometry[(Respirometry.Training == training)\n",
    "                                                                              & (Respirometry.Experiment == 'Control')].o2.dropna(),\n",
    "                                                                 Respirometry[(Respirometry.Training == training)\n",
    "                                                                              & (Respirometry.Experiment == 'Swimmer')].o2.dropna(),\n",
    "                                                                 equal_var=True)\n",
    "    print(\"At %s, the 'o2' for 'Control' and 'Swimmer' have a\" % training)\n",
    "    # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "    p_value_respirometry[c] /= 2\n",
    "    print('F value of %0.4s and a p value of %.2g (%s).' % (t_statistic,\n",
    "                                                            p_value_respirometry[c],\n",
    "                                                            significance(p_value_respirometry[c])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the 02 consumptions between trainings different?\n",
    "for c, experiment in enumerate(Respirometry.Experiment.unique()):\n",
    "    for trainings in itertools.combinations(Respirometry.Training.unique(), 2):\n",
    "        print(80 * '-')\n",
    "        t_statistic, p_value_respirometry[c + 2] = scipy.stats.ttest_ind(Respirometry[(Respirometry.Training == trainings[0])\n",
    "                                                                                      & (Respirometry.Experiment == experiment)].o2.dropna(),\n",
    "                                                                         Respirometry[(Respirometry.Training == trainings[1])\n",
    "                                                                                      & (Respirometry.Experiment == experiment)].o2.dropna(),\n",
    "                                                                         equal_var=True)\n",
    "        print(\"For the %s group, the '02' between '%s' and '%s' has an\" % (experiment,\n",
    "                                                                           trainings[0],\n",
    "                                                                           trainings[1]))\n",
    "        # Two-sided test done, but we want one-sided test --> p_value / 2 (only because we tested it above)\n",
    "        p_value_respirometry[c + 2] /= 2\n",
    "        print('F value of %0.4s and a p value of %.2g. (%s)' % (t_statistic,\n",
    "                                                                p_value_respirometry[c + 2],\n",
    "                                                                significance(p_value_respirometry[c + 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display respirometry data\n",
    "seaborn.violinplot(data=Respirometry, hue='Experiment', y='o2', x='Training', cut=0, inner='quartiles')\n",
    "# seaborn.boxplot(data=Respirometry, hue='Experiment', y='o2', x='Training', showfliers=False, notch=True)\n",
    "seaborn.stripplot(data=Respirometry, hue='Experiment', y='o2', x='Training',\n",
    "                  jitter=True, dodge=True, s=5, linewidth=1.5,\n",
    "                  palette=['gray', 'gray'])\n",
    "if legend:\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles[:len(handles) // 2],\n",
    "               labels[:len(labels) // 2],\n",
    "               loc='lower right')\n",
    "else:\n",
    "    plt.legend().set_visible(False)\n",
    "plt.xlabel('')\n",
    "plt.gca().set_xticklabels(['Before Training', 'After Training'])\n",
    "plt.ylabel('Normalized $O_{2}$ consumption')\n",
    "significance_bar(1 - 0.2, 1 + 0.2,\n",
    "                 Respirometry['o2'].max(),\n",
    "                 p_value_respirometry[1])\n",
    "seaborn.despine(offset=10, trim=True, bottom=True)\n",
    "plt.savefig(os.path.join(OutPutDir, 'O2_normalized.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Data.Experiment.unique():\n",
    "    print('The normalized O2 consumption for '\n",
    "          'the %s group at start was '\n",
    "          '%0.4f with a STD of %0.4f.' % (experiment,\n",
    "                                          Data[Data.Experiment == experiment]['O2 consumption start normalized'].mean(),\n",
    "                                          Data[Data.Experiment == experiment]['O2 consumption start normalized'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is an increase (control to swimmer) '\n",
    "      'of %0.2f %%' % percentage(Data[Data.Experiment == 'Control']['O2 consumption start normalized'].mean(),\n",
    "                                 Data[Data.Experiment == 'Swimmer']['O2 consumption start normalized'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Data.Experiment.unique():\n",
    "    print('The normalized O2 consumption for '\n",
    "          'the %s group at end was '\n",
    "          '%0.4f with a STD of %0.4f.' % (experiment,\n",
    "                                          Data[Data.Experiment == experiment]['O2 consumption end normalized'].mean(),\n",
    "                                          Data[Data.Experiment == experiment]['O2 consumption end normalized'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is an increase (control to swimmer) '\n",
    "      'of %0.2f %%' % percentage(Data[Data.Experiment == 'Swimmer']['O2 consumption end normalized'].mean(),\n",
    "                                 Data[Data.Experiment == 'Control']['O2 consumption end normalized'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Data.Experiment.unique():\n",
    "    print('Increase of %s (start to end) '\n",
    "          'is %0.2f %%' % (experiment,\n",
    "                           percentage(Data[Data.Experiment == experiment]['O2 consumption start normalized'].mean(),\n",
    "                                      Data[Data.Experiment == experiment]['O2 consumption end normalized'].mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data at the end\n",
    "Data.to_csv('Data_%s.csv' % get_git_hash())\n",
    "Data.to_csv(os.path.join(OutPutDir,\n",
    "                         'Data_%s.csv' % get_git_hash()))\n",
    "# Speed.to_csv('Speed_%s.csv' % get_git_hash())\n",
    "Speed.to_csv(os.path.join(OutPutDir,\n",
    "                          'Speed_%s.csv' % get_git_hash()))\n",
    "# Morphology.to_csv('Morphology_%s.csv' % get_git_hash())\n",
    "Morphology.to_csv(os.path.join(OutPutDir,\n",
    "                               'Morphology_%s.csv' % get_git_hash()))\n",
    "# Filaments.to_csv('Filaments_%s.csv' % get_git_hash())\n",
    "Filaments.to_csv(os.path.join(OutPutDir,\n",
    "                              'Filaments_%s.csv' % get_git_hash()))\n",
    "# Respirometry.to_csv('Respirometry_%s.csv' % get_git_hash())\n",
    "Respirometry.to_csv(os.path.join(OutPutDir,\n",
    "                                 'Respirometry_%s.csv' % get_git_hash()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All data has been saved to %s' % os.path.join(OutPutDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
